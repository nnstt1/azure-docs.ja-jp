---
title: 用語 - Personalizer
description: Personalizer では、強化学習の用語を使用します。 これらの用語は、Azure portal と API で使用されます。
author: jeffmend
ms.author: jeffme
ms.manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 04/23/2020
ms.openlocfilehash: bdd26d167a94826be4330f878df01cde43e853cb
ms.sourcegitcommit: 16e25fb3a5fa8fc054e16f30dc925a7276f2a4cb
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/25/2021
ms.locfileid: "122829921"
---
# <a name="personalizer-terminology"></a>Personalizer の用語

Personalizer では、強化学習の用語を使用します。 これらの用語は、Azure portal と API で使用されます。

## <a name="conceptual-terminology"></a>概念的な用語

* **学習ループ**: パーソナル化によってメリットが得られるアプリケーションのあらゆる部分に対して、"_学習ループ_" と呼ばれる Personalizer リソースを作成します。 パーソナル化するエクスペリエンスが複数ある場合は、それぞれにループを作成します。

* **モデル**:Personalizer モデルは、ユーザーの動作に関して学習したすべてのデータを取得し、Rank と Reward の呼び出しに送信した引数と学習ポリシーで決定されたトレーニング動作の組み合わせからトレーニング データを取得します。

* **オンライン モード**: Personalizer の既定の [学習動作](#learning-behavior)である "学習ループ" では、機械学習を使用して、コンテンツの **最上位のアクション** を予測するモデルが構築されます。

* **見習いモード**:アプリケーションの結果とアクションに影響を与えることなく Personalizer モデルのトレーニングをウォームスタートできる [学習動作](#learning-behavior)です。

## <a name="learning-behavior"></a>学習動作:

* **オンライン モード**: 最適なアクションが返されます。 モデルでは、最適なアクションを使用して Rank 呼び出しに応答し、Reward 呼び出しを使用して学習し、時間の経過とともに選択内容が改善されていきます。
* **[見習いモード](concept-apprentice-mode.md)** : 初心者として学習します。 モデルでは、既存のシステムの動作を観察することによる学習が行われます。 Rank 呼び出しでは、常にアプリケーションの **既定のアクション** (ベースライン) が返されます。

## <a name="personalizer-configuration"></a>Personalizer の構成

Personalizer は、[Azure portal](https://portal.azure.com) から構成します。

* **報酬**: 報酬の待機時間、既定の報酬、および報酬の集計ポリシーの既定値を構成します。

* **探索**: 探索に使用する Rank 呼び出しの割合を構成します

* **モデルの更新頻度**:モデルが再トレーニングされる頻度。

* **データ保有期間**:データを保存する日数。 これは、学習ループの向上に使用されるオフライン評価に影響を与える可能性があります。

## <a name="use-rank-and-reward-apis"></a>Rank および Reward API を使用する

* **Rank**: 特徴を含むアクションとコンテキストの特徴を考慮して、探索または活用を使用して、最上位のアクション (コンテンツ項目) を返します。

    * **Actions**:アクションは、商品やプロモーションなど、選択対象のコンテンツ項目です。 Personalizer は、Rank API を介してユーザーに表示する最上位のアクション (返される報酬アクション ID) を選択します。

    * **コンテキスト**:より正確な順位を提供するために、コンテキストに関する情報を提供します。次に例を示します。
        * ユーザー。
        * ユーザーが使用しているデバイス。
        * 現在の時刻。
        * 現在の状況に関するその他のデータ。
        * ユーザーまたはコンテキストに関する履歴データ。

        特定のアプリケーションにおいて異なるコンテキスト情報がある場合があります。

    * **[特徴](concepts-features.md)** : コンテンツ項目またはユーザー コンテキストに関する情報のユニット。 集計された特徴のみを使用するようにしてください。 特定の時刻、ユーザー ID、その他の未集計データを特徴として使用しないでください。

        * "_アクションの特徴_" は、コンテンツに関するメタデータです。
        * "_コンテキストの特徴_" は、コンテンツが表示されるコンテキストに関するメタデータです。

* **探索**: Personalizer サービスは、最善のアクションを返す代わりに、ユーザーに対して別のアクションを選択するときに探索を行っています。 Personalizer サービスは、ドリフトや停滞を回避し、探索することで進行中のユーザーの動作に適応できます。

* **活用**:Personalizer サービスでは、現在のモデルを使用して、過去のデータに基づく最善のアクションを決定します。

* **実験期間**: そのイベントに対して Rank 呼び出しが行われた時点からの、Personalizer サービスが報酬を待つ時間の長さ。

* **非アクティブなイベント**: 非アクティブなイベントとは、Rank が呼び出されたときにクライアント アプリケーションによる決定によりユーザーに結果が表示されるかどうかが不明なイベントを表します。 非アクティブなイベントを使用すると、パーソナル化の結果を作成して保存した後、機械学習モデルに影響を与えることなくそれらを破棄することを決定できます。


* **報酬**: Rank API の返された報酬アクション ID に対してユーザーがどのように応答したかを示すメジャー (0 から 1 のスコア)。 0 から 1 の値は、その選択がパーソナル化のビジネス目標の達成にどのように役立ったかに基づいて、ビジネス ロジックによって設定されます。 学習ループでは、この報酬は個々のユーザー履歴として格納されません。

## <a name="evaluations"></a>評価

### <a name="offline-evaluations"></a>オフライン評価

* **評価**:オフライン評価では、アプリケーションのデータに基づいて、ループに最適な学習ポリシーが決定されます。

* **学習ポリシー**:Personalizer によるすべてのイベントに対するモデルのトレーニング方法は、機械学習アルゴリズムの動作方法に影響するいくつかのパラメーターによって決まります。 新しい学習ループは既定の **学習ポリシー** から始まります。これにより、適度なパフォーマンスが得られます。 [評価](concepts-offline-evaluation.md)を実行すると、Personalizer では、お使いのループのユース ケースに合わせて特別に最適化された新しい学習ポリシーが作成されます。 評価時に生成された特定のループごとに最適化されたポリシーを使用すると、Personalizer ははるかに優れたパフォーマンスを発揮します。 この学習ポリシーの名前は、Azure portal の Personalizer リソースに対する **[モデルと学習設定]** の "_学習設定_" です。

### <a name="apprentice-mode-evaluations"></a>見習いモードでの評価

見習いモードでは、次の **評価メトリック** が提供されます。
* **ベースライン - 平均報酬**: アプリケーションの既定値 (ベースライン) の平均報酬。
* **Personalizer - 平均報酬**: Personalizer で達成している可能性がある報酬合計の平均。
* **平均ローリング報酬**: ベースライン報酬と Personalizer 報酬の比率 – 最新の 1000 件のイベントで正規化されます。

## <a name="next-steps"></a>次のステップ

* [倫理と責任ある使用](ethics-responsible-use.md)