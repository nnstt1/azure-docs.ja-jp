---
title: 見習いモード - Personalizer
description: 見習いモードを使用して、コードを変更せずにモデルの信頼度を高める方法について説明します。
author: jeffmend
ms.author: jeffme
ms.manager: nitinme
ms.service: cognitive-services
ms.subservice: personalizer
ms.topic: conceptual
ms.date: 05/01/2020
ms.openlocfilehash: aabda26cd2a1dea7205c8432f11af35e3f20b0e1
ms.sourcegitcommit: 16e25fb3a5fa8fc054e16f30dc925a7276f2a4cb
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/25/2021
ms.locfileid: "122830875"
---
# <a name="use-apprentice-mode-to-train-personalizer-without-affecting-your-existing-application"></a>見習いモードを使用して、既存のアプリケーションに影響を与えずに Personalizer をトレーニングする

**実際** の補強学習の性質により、Personalizer モデルは運用環境でのみトレーニングできます。 新しいユース ケースをデプロイする場合、モデルが十分にトレーニングされるまで時間がかかるため、Personalizer モデルは効率的には実行されません。  **見習いモード** は、このような状況を軽減し、開発者がコードを変更することなくモデルの自信を得ることができる学習動作です。

[!INCLUDE [Important Blue Box - Apprentice mode pricing tier](./includes/important-apprentice-mode.md)]

## <a name="what-is-apprentice-mode"></a>見習いモードとは

見習いが熟練の職人から技術を学び、経験によってさらに向上するのと同じように、徒弟モードは、既存のアプリケーション ロジックから取得された結果を観察することによって Personalizer が学習できるようにする "_動作_" です。

Personalizer は、アプリケーションと同じ出力を模倣することによってトレーニングを行います。 イベント フローが増えると、Personalizer は既存のロジックと結果に影響を与えることなく、既存のアプリケーションに "_追い付く_" ことができます。 Azure portal と API から使用できるメトリックは、モデルが学習するパフォーマンスを把握するのに役立ちます。

Personalizer が一定のレベルの理解を学習して獲得すると、開発者は動作を見習いモードからオンライン モードに変更できます。 その時点で、Personalizer は Rank API でのアクションに影響を与えるようになります。

## <a name="purpose-of-apprentice-mode"></a>見習いモードの目的

見習いモードを使用すると、Personalizer サービスとその機械学習機能が信頼できるようになり、オンライン トラフィックを危険にすることなく、学習可能な情報がサービスに送信されることの再保証が提供されます。

見習いモードを使用する主な理由は、次の 2 つです。

* **コールド スタート** の軽減: 見習いモードは、最良のアクションが返されず、60 - 80% 前後の十分なレベルの効果が得られない場合に、"新しい" モデルの学習時間を管理および評価するのに役立ちます。
* **アクションとコンテキストの特徴の検証**: アクションとコンテキストで送信される特徴は、少なすぎる、多すぎる、正しくない、詳細すぎるなど、理想的な有効度を達成するために Personalizer をトレーニングするのに不適切または不正確である場合があります。 特徴に関する問題を見つけて修正するには、[特徴評価](concept-feature-evaluation.md)を使用します。

## <a name="when-should-you-use-apprentice-mode"></a>どのようなときに見習いモードを使用する必要があるか

ユーザーのエクスペリエンスが Personalizer によって影響を受けないようにしながら、次のシナリオを通して有効性が向上するように Personalizer をトレーニングするには、見習いモードを使用します。

* 新しいユース ケースに Personalizer を実装しています。
* コンテキストまたはアクションで送信する特徴を大幅に変更しました。
* 報酬を計算するタイミングと方法を大幅に変更しました。

見習いモードは、報酬スコアに対する Personalizer の影響を測定する効果的な方法ではありません。 各 Rank の呼び出しに対して最適なアクションの選択での Personalizer の効果を測定するには、[オフライン評価](concepts-offline-evaluation.md)を使用します。

## <a name="who-should-use-apprentice-mode"></a>見習いモードを使用する必要があるユーザー

見習いモードは、開発者、データ サイエンティスト、およびビジネス上の意思決定者に役立ちます。

* **開発者** は、見習いモードを使用して、Rank API と Reward API がアプリケーションで正しく使用されていること、およびアプリケーションから Personalizer に送信される特徴にバグ、またはタイムスタンプや UserID 要素などの関連のない特徴が含まれないことを、確認することができます。

* **データ サイエンティスト** は、見習いモードを使用して、特徴が Personalizer モデルのトレーニングに効果的であること、および報酬の待機時間が長すぎたり短かすぎたりしないことを、検証できます。

* **ビジネス上の意思決定者** は、見習いモードを使用して、既存のビジネス ロジックと比較して Personalizer により結果 (つまり報酬) が向上する可能性を評価することができます。 これにより、実際の収益やユーザー満足度につながるユーザー エクスペリエンスに影響を与える決定を、情報に基づいて行うことができます。

## <a name="comparing-behaviors---apprentice-mode-and-online-mode"></a>動作の比較 - 見習いモードとオンライン モード

見習いモードでの学習は、次の点がオンライン モードと異なります。

|領域|見習いモード|オンライン モード|
|--|--|--|
|ユーザー エクスペリエンスへの影響|既存のユーザー動作を使用し、**既定のアクション** とそれによって獲得された報酬を(影響を与えるのではなく) 観察することによって、Personalizer をトレーニングできます。 これは、ユーザーのエクスペリエンスとビジネスの結果が影響を受けないことを意味します。|Rank の呼び出しから返された上位のアクションを表示して、ユーザーの動作に影響を与えます。|
|学習速度|Personalizer の学習速度は、オンライン モードで学習しているときより、見習いモードのときの方が遅くなります。 見習いモードでは、**既定のアクション** によって獲得される報酬を観察することによってのみ学習できます。そのため、探索を実行できないので、学習の速度が制限されます。|現在のモデルの活用と新しい傾向の調査の両方が可能であるため、より迅速に学習できます。|
|学習効果の "天井"|Personalizer は、基になるビジネス ロジックを近似でき、一致することはほとんどなく、超えることはありません (各 Rank 呼び出しの **既定のアクション** によって獲得される報酬合計)。 この近似値の限界は探索によって軽減されます。 たとえば、20% の探索では、見習いモードのパフォーマンスが 80% を超えることはほとんどなく、60% がオンライン モードにレベル アップするための妥当な目標になります。|Personalizer はアプリケーションのベースラインを超える必要があり、時間が経って行き詰まったら、オフライン評価と特徴評価を実施して、モデルの改善を続ける必要があります。 |
|rewardActionId に対する Rank API の値|_rewardActionId_ は常に Rank 要求で送信する最初のアクションであるため、ユーザーのエクスペリエンスが影響を受けることはありません。 つまり、見習いモードの間は、Rank API はアプリケーションに対して目に見えることは何も行いません。 アプリケーションの Reward API は、モードが変わっても、Reward API の使用方法が変更されないようにする必要があります。|ユーザーのエクスペリエンスは、Personalizer によってアプリケーションに対して選択された _rewardActionId_ により変更されます。 |
|評価|Personalizer では、既定のビジネス ロジックによって得られる報酬の合計と、その時点でオンライン モードになっている場合に Personalizer が獲得する報酬の合計の比較が保持されています。 比較は、そのリソースの Azure portal で使用できます|[オフライン評価](concepts-offline-evaluation.md)を実行することによって、Personalizer の有効性を評価します。これにより、Personalizer によって達成された合計報酬と、アプリケーションのベースラインの予想される報酬を比較できます。|

見習いモードの有効性に関する注意事項:

* 見習いモードでの Personalizer の有効性は、アプリケーションのベースラインの 100% 近くを達成することはほとんどなく、それを超えることはありません。
* ベスト プラクティスは、100% の達成を試みないようにすることです。ただし、ユース ケースによっては、60 から 80% の範囲を対象にする必要があります。

## <a name="limitations-of-apprentice-mode"></a>徒弟モードの制限事項
徒弟モードでは、コンテキストに存在する特徴と、Rank 呼び出しで使用されるアクションおよび Reward 呼び出しからのフィードバックを使用して、ベースライン項目を選択する既存のアルゴリズムを模倣しようとすると、Personalizer モデルをトレーニングしようとします。 Personalizer の徒弟が一致した報酬を十分に学習する場合に、次の要因が影響します。

### <a name="scenarios-where-apprentice-mode-may-not-be-appropriate"></a>徒弟モードが適切ではない可能性があるシナリオ:

#### <a name="editorially-chosen-content"></a>編集によって選択されたコンテンツ:
ニュースやエンターテイメントなどの一部のシナリオでは、編集チームによってベースライン項目を手動で割り当てることができます。 つまり、人間は、より広範な世界に関する知識を用い、何が魅力的なコンテンツになるか認識して、代表取材から特定の記事やメディアを選択し、「優先」や「ヒーロー」の記事としてフラグを設定しています。 このようなエディターはアルゴリズムではなく、エディターによって考慮する要素が微妙に異なり、コンテキストとアクションの特徴として含まれていない可能性が高いので、徒弟モードでは次のベースライン アクションを予測できる可能性が低いと考えられます。 このような状況では、次のことが可能です。

* オンライン モードでの Personalizer のテスト: ベースラインを予測していない徒弟モードでは、Personalizer は良い結果やより良い結果を達成できないことになります。 Personalizer を一定期間オンライン モードにするか、インフラストラクチャがある場合は A/B テストに入れたのち、オフライン評価を実行して違いを評価することを検討します。
* 編集上の考慮事項と推奨事項を特徴として追加する: 選択に影響を与える要因をエディターに確認し、コンテキストとアクションで特徴として追加できるのかを確認します。 たとえば、メディア会社の編集者は、特定の著名人が話題になっている間は、コンテンツを大きく取り上げることがあります。この知識はコンテキストの特徴として追加できます。

### <a name="factors-that-will-improve-and-accelerate-apprentice-mode"></a>徒弟モードを強化して学習を加速させる要因
徒弟モードで学習して一致した報酬を 0 より上で獲得しているが、成長が遅いような場合 (60% に達していない、2 週間以内の一致した報酬が 80%)、課題のデータが少なすぎる可能性があります。 次の手順を実行すると、学習が加速する可能性があります。 

1. 時間をかけてプラスの報酬のあるイベントを追加する: 徒弟モードでは、アプリケーションが 1 日あたり 100 以上のプラスの報酬を得るとパフォーマンスが向上します。 たとえば、クリックに報酬を与える Web サイトのクリックスルーが 2% の場合、顕著な学習を行うには、1 日あたり少なくとも 5,000 回のアクセスが必要です。 
2. また、簡単で頻繁に得られる報酬スコアを試してみてください。 たとえば、「ユーザーは記事の読み取りを完了しましたか」から「ユーザーは記事の読み始めでしたか」に変更します。
3. 差別化する特徴を追加する: Rank 呼び出しのアクションとその特徴を視覚的に検査できます。 ベースライン アクションには、他のアクションと区別される特徴がありますか? ほとんどが同じように見える場合は、類似性を低くする特徴を追加します。
4. イベントごとのアクションを減らす: Personalizer では、[探索率] 設定を使用して、好みと傾向を見つけます。 Rank 呼び出しにさらに多くのアクションがある場合、探索に対するアクションが選択される可能性が低くなります。 各 Rank 呼び出しで送信されるアクションの数を 10 未満に減らします。 これは、徒弟モードのデータが報酬と一致することを示す一時的な調整になります。


## <a name="using-apprentice-mode-to-train-with-historical-data"></a>見習いモードを使用して履歴データでトレーニングを行う

大量の履歴データがあり、Personalizer のトレーニングに使用したい場合は、見習いモードを使用し、Personalizer を通してデータを再生できます。

見習いモードで Personalizer を設定し、履歴データからのアクションとコンテキストの特徴で Rank を呼び出すスクリプトを作成します。 このデータのレコードの計算に基づいて、Reward API を呼び出します。 何らかの結果を得るには約 5 万件の履歴イベントが必要ですが、結果の信頼性を高めるために 50 万件が推奨されます。

履歴データからトレーニングするときは、送信されるデータ (コンテキストとアクションの特徴、Rank 要求で使用される JSON のレイアウト、このトレーニング データ セットでの報酬の計算) を、既存のアプリケーションから利用できるデータ (特徴および特典の計算) と一致させることをお勧めします。

オフライン データと事後データは、より不完全で、形式の不備や相違が多くなる傾向があります。 履歴データからのトレーニングを行うことは可能ですが、その結果は決定的でなく、Personalizer の学習の結果を予測するのに適していない可能性があります。過去のデータと既存のアプリケーションで特徴が異なる場合は特にそうです。

一般に、Personalizer では、履歴データを使用してトレーニングするより、見習いモードに動作を変更し、既存のアプリケーションから学習した方が、効果的なモデルを作成するためのより効率的なパスになり、作業量、Data Engineering、クリーンアップ作業が少なくなります。

## <a name="using-apprentice-mode-versus-ab-tests"></a>見習いモードの使用と A/B テストの使用

検証が済み、オンライン モードで学習している場合にのみ、Personalizer の処理の A/B テストを行うと役に立ちます。 見習いモードでは、**既定のアクション** のみが使用されます。これは、すべてのユーザーに対してコントロール エクスペリエンスが効果的に表示されることを意味します。

Personalizer が "_処理_" だけであっても、データの検証が Personalizer のトレーニングに対して適切であるときは、同じ課題が存在します。 100% のトラフィックを使用し、すべてのユーザーがコントロール (影響を受けない) エクスペリエンスを取得するようにして、代わりに見習いモードを使用できます。

Personalizer を使用してオンラインで学習するユース ケースができたら、A/B 実験を使用して、報酬に使用されるシグナルより複雑な可能性がある結果の制御されたコーホートと科学的な比較を行うことができます。 A/B テストで回答できる質問の例を次に示します: `In a retail website, Personalizer optimizes a layout and gets more users to _check out_ earlier, but does this reduce total revenue per transaction?`

## <a name="next-steps"></a>次のステップ

* [アクティブなイベントと非アクティブなイベント](concept-active-inactive-events.md)について学習します
