---
title: カスタム分類の評価メトリック
titleSuffix: Azure Cognitive Services
description: カスタム エンティティ抽出の評価メトリックについて説明します。
services: cognitive-services
author: aahill
manager: nitinme
ms.service: cognitive-services
ms.subservice: language-service
ms.topic: overview
ms.date: 11/02/2021
ms.author: aahi
ms.custom: language-service-custom-classification, ignite-fall-2021
ms.openlocfilehash: 53765ebe1862b4c46a1b4ad8caf75f64f6203e37
ms.sourcegitcommit: 106f5c9fa5c6d3498dd1cfe63181a7ed4125ae6d
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/02/2021
ms.locfileid: "131091507"
---
# <a name="evaluation-metrics"></a>評価メトリック

[データセットは 2 つに分割](../how-to/train-model.md#data-splits)されています (トレーニング用セットとテスト用セット)。 トレーニング用セットはモデル構築時に、テスト用セットはトレーニング完了後にモデルのパフォーマンスを評価するためのブラインド セットとして使用されます。

モデルの評価は、トレーニングが正常に完了した後にトリガーされます。 評価プロセスでは、トレーニング済みモデルを使い、テスト セット内のファイルのユーザー定義クラスを予測することから始まり、指定されたデータ タグ (真実のベースラインを確立するもの) との比較を行います。 その結果が返されるので、モデルのパフォーマンスを確認することができます。 カスタム テキスト分類の評価には、次のメトリックを使用します。

* **精度**: モデルの精密さと正確さを測定します。 これは、正しく識別された陽性 (真陽性) と識別されたすべての陽性との比率です。 精度メトリックによって、予測クラスのうち、正しくタグが付けられている数が明らかになります。 

    `Precision = #True_Positive / (#True_Positive + #False_Positive)`

* **リコール**: 実際の陽性クラスを予測するモデルの能力を測定します。 これは、予測された真陽性と実際にタグが付けられたものとの比率です。 リコール メトリックによって、予測クラスのうち、正しいものの数が明らかになります。

    `Recall = #True_Positive / (#True_Positive + #False_Negatives)`

* **F1 スコア**: F1 スコアは、精度とリコールの関数です。 精度とリコールのバランスを取るときに必要です。

    `F1 Score = 2 * Precision * Recall / (Precision + Recall)` <br> 

>[!NOTE]
> 精度、リコール、F1 スコアは、各クラスについては個別に計算され ("*クラスレベル*" の評価)、モデルについてはまとめて計算されます ("*モデルレベル*" の評価)。

## <a name="model-level-and-class-level-evaluation-metrics"></a>モデルレベルとクラスレベルの評価メトリック

精度、リコール、評価の定義は、クラスレベルとモデルレベルのどちらの評価でも同じです。 ただし、次の例に示すように、"*真陽性*"、"*擬陽性*"、"*擬陰性*" の数は異なります。

以下のセクションでは、次のデータセット例を使用します。

| File | 実際のクラス | 予測クラス |
|--|--|--|
| 1 | アクション、コメディ | コメディ|
| 2 | action | action |
| 3 | ロマンス | ロマンス |
| 4 | ロマンス、コメディ | ロマンス |
| 5 | コメディ | action |

### <a name="class-level-evaluation-for-the-action-class"></a>"*アクション*" クラスのクラスレベル評価 

| Key | Count | 説明 |
|--|--|--|
| 真陽性 | 1 | ファイル 2 は正しく "*アクション*" に分類されました。 |
| 偽陽性 | 1 | ファイル 5 は誤って "*アクション*" に分類されました。 |
| 偽陰性 | 1 | ファイル 1 は "*アクション*" に分類されるべきでしたが、そうなりませんでした。 |

**精度** = `#True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 1) = 0.5`

**リコール** = `#True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 1) = 0.5`

**F1 スコア** = `2 * Precision * Recall / (Precision + Recall) =  (2 * 0.5 * 0.5) / (0.5 + 0.5) = 0.5`

### <a name="class-level-evaluation-for-the-comedy-class"></a>"*コメディ*" クラスのクラスレベル評価 

| Key | Count | 説明 |
|--|--|--|
| 真陽性 | 1 | ファイル 1 は正しく "*コメディ*" に分類されました。 |
| 偽陽性 | 0 | 誤って "*コメディ*" に分類されたファイルはありませんでした。 |
| 偽陰性 | 2 | ファイル 5 と 4 は "*コメディ*" に分類されるべきでしたが、そうなりませんでした。 |

**精度** = `#True_Positive / (#True_Positive + #False_Positive) = 1 / (1 + 0) = 1`

**リコール** = `#True_Positive / (#True_Positive + #False_Negatives) = 1 / (1 + 2) = 0.67`

**F1 スコア** = `2 * Precision * Recall / (Precision + Recall) =  (2 * 1 * 0.67) / (1 + 0.67) = 0.8`

### <a name="model-level-evaluation-for-the-collective-model"></a>集合モデルのモデル レベルの評価

| Key | Count | 説明 |
|--|--|--|
| 真陽性 | 4 | ファイル 1、2、3、4 は、予測時に正しいクラスが指定されました。 |
| 偽陽性 | 1 | ファイル 5 は、予測時に間違ったクラスが指定されました。 |
| 偽陰性 | 2 | ファイル 1 と 4 は、予測時にすべての正しいクラスが指定されませんでした。 |

**精度** = `#True_Positive / (#True_Positive + #False_Positive) = 4 / (4 + 1) = 0.8`

**リコール** = `#True_Positive / (#True_Positive + #False_Negatives) = 4 / (4 + 2) = 0.67`

**F1 スコア** = `2 * Precision * Recall / (Precision + Recall) =  (2 * 0.8 * 0.67) / (0.8 + 0.67) = 0.12`

> [!NOTE] 
> 単一ラベル分類モデルの場合、偽陰性と擬陽性の数は常に等しくなります。 カスタム単一ラベル分類モデルの場合、各ファイルに対して常に 1 つのクラスが予測されます。 予測が正しくない場合、予測クラスの FP 数が 1 つ増え、実際のクラスの FN 数が 1 つ増えますが、モデル全体の FP と FN の数は常に等しくなります。 これは、マルチラベル分類には当てはまりません。ファイルのクラスのいずれかを予測できなかった場合は、擬陰性としてカウントされるためです。 

## <a name="interpreting-class-level-evaluation-metrics"></a>クラスレベルの評価メトリックの解釈

それでは、特定のクラスに対する精度またはリコールが高いというのは、実際にはどのような意味なのでしょうか。

| 呼び戻し | Precision | 解釈 |
|--|--|--|
| 高 | 高 | このクラスは、このモデルによって完全に処理されます。 |
| 低 | 高 | このクラスは、このモデルによって予測されないことがありますが、実行される場合は信頼度は高くなります。 これは、このクラスがデータセット内に少数しか存在しないことが原因の可能性があるため、データ分布のバランスを取ることを検討してください。|
| 高 | 低 | このモデルを使用すると、このクラスを適切に予測できますが、信頼度は低くなります。 これは、このクラスがデータセット内に多数存在することが原因の可能性があるため、データ分布のバランスを取ることを検討してください。 |
| 低 | 低 | このクラスは、このモデルによって適切に処理されません。通常は予測されず、されたとしても信頼度は高くありません。 |

カスタム分類モデルでは、偽陰性と擬陽性の両方が発生することが予想されます。 それぞれがシステム全体にどのような影響するかを考慮し、モデルによって正しい予測が無視されたり、間違った予測が認識されたりするシナリオを慎重に検討する必要があります。 シナリオによっては、"*精度*" または "*リコール*" の方がモデルのパフォーマンス評価に適している可能性があります。  

たとえば、テクニカル サポートのチケットの処理に関するシナリオの場合、間違ったクラスを予測すると、間違った部門やチームに転送される可能性があります。 この例では、システムを擬陽性に対する感度を高めることを検討する必要があり、評価には精度がより適したメトリックとなります。 

もう 1 つの例として、メールを "*重要*" と "*スパム*" に分類するシナリオの場合、誤った予測で "*スパム*" とラベルが付けられると、有益なメールを見逃す可能性があります。 一方、スパム メールに "*重要*" とラベルが付けられた場合は、無視するだけで済みます。 この例では、システムを偽陰性に対する感度を高めることを検討する必要があり、評価にはリコールがより適したメトリックとなります。 

汎用的なシナリオに最適化する場合や、精度とリコールの両方が重要な場合は、F1 スコアを利用できます。 評価スコアは、シナリオや受け入れ基準に基づいた主観的なものです。 すべてのシナリオに通用する絶対的な評価メトリックはありません。 

## <a name="see-also"></a>関連項目

* [モデルの評価を表示する](../how-to/view-model-evaluation.md)
* [モデルをトレーニングする](../how-to/train-model.md)
