---
title: Azure Video Analyzer パイプライン
description: Azure Video Analyzer のエッジとクラウド内でビデオを取り込み、プロセスし、発行する Azure Video Analyzer パイプライン。 パイプラインは、目的のデータ フローを実現するために接続されているノードで構成されます。
ms.topic: conceptual
ms.date: 11/04/2021
ms.custom: ignite-fall-2021
ms.openlocfilehash: 261cbc17f8bd54d16c10b3f97a997cb6ac594fdb
ms.sourcegitcommit: 677e8acc9a2e8b842e4aef4472599f9264e989e7
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/11/2021
ms.locfileid: "132287155"
---
# <a name="pipeline"></a>パイプライン

Pipelines、Azure Video Analyzer のエッジとクラウド内でビデオを取り込み、プロセス、発行できます。 パイプライン トポロジを使用すると、構成可能なノードのセットを使用して、ビデオを取り込み、処理し、公開する方法を定義できます。 定義したトポロジは、個別に処理される特定のカメラまたはソース コンテンツを対象とする個々のパイプラインとしてインスタンス化できます。 Pipelinesは、オンプレミスのビデオ処理またはクラウドのエッジで定義およびインスタンス化できます。 次の図は、このようなパイプラインのグラフィカルな表現を示しています。

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/pipeline/pipeline-representation.svg" alt-text="パイプラインの表現":::

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/pipeline/cloud-pipeline.svg" alt-text="クラウド パイプラインの表現":::

## <a name="suggested-pre-reading"></a>先に読んでおくことが推奨される記事

* [概要](overview.md)
* [用語](terminology.md)

## <a name="pipeline-topologies"></a>パイプライン技術

[パイプライン トポロジ](terminology.md#recording)を使用すると、一連の相互接続されたノードを使用して、カスタム ニーズに合ったライブ ビデオまたは記録されたビデオを処理および分析する方法を記述できます。 ライブ ビデオ ワークフローのテンプレートまたはブループリントとして機能します。 **ビデオ アナライザーでは、ライブとバッチの 2 種類のトポロジがサポートされています。名前が示すように、ライブ トポロジはカメラからのライブ ビデオと一緒に使用されます。バッチ トポロジは、記録されたビデオを処理するために使用されます。**

**パイプラインでは** 、さまざまな種類のノードがサポートされています。

* **ソース ノード** は、パイプラインへのデータのキャプチャを有効にします。 データとは、オーディオ、ビデオ、メタデータを指します。
* **プロセッサ ノード** は、パイプライン内のメディアの処理を有効にします。
* **シンク ノード** は、パイプラインの外部にあるサービスとアプリに結果を配信できます。

トポロジ内のノード、接続方法、値のプレースホルダーとしてパラメーターを選択することで、さまざまなシナリオに対して異なるトポロジを作成できます。 パイプラインは、特定のパイプライン技術についての個別インスタンスです。 パイプラインとは、メディアが実際に処理される場所です。 パイプラインは、パイプライン トポロジで宣言されたユーザー定義パラメーターを使用して、個々のカメラ (および他の側面) に関連付けることができます。 **ライブ トポロジのインスタンスはライブ パイプラインと呼ばれます。バッチ トポロジのインスタンスはパイプライン ジョブと呼ばれます。**

たとえば、複数の IP カメラからビデオを記録する場合は、RTSP ソース ノードとビデオ シンク ノードで構成されるパイプライン トポロジを定義できます。 RTSP ソース ノードには、パラメーターとして RTSP URL、ユーザー名、およびパスワードを指定できます。 ビデオ シンク ノードには、パラメーターとしてビデオ名を指定できます。 これらのパラメーターの値は、同じトポロジ (カメラ 1 台につき 1 つのパイプライン) から複数のパイプラインを作成するときに指定できます。

バッチ トポロジは、Video Analyzer サービスでのみサポートされます (Video Analyzer エッジ モジュールではサポートされません)。 ライブ パイプラインは両方でサポートされています。

## <a name="pipeline-states"></a>パイプラインの状態

まず、パイプライン のトポロジを作成します。 トポロジを定義したら、パラメーターの値を指定してパイプラインを作成できます。

### <a name="live-pipeline"></a>ライブ パイプライン

パイプラインのライフサイクルを次の図に示します。

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/pipeline/pipeline-activation.svg" alt-text="ライブ パイプラインのライフサイクル":::

正常に作成されると、パイプラインは「非アクティブ」状態になります。 アクティブ化が行われると、"アクティブ" 状態になる前にパイプラインが "アクティブ化" 状態になります。
データ (ライブ ビデオ) は、「アクティブ」状態に到達すると、パイプラインを介してフローを開始します。 非アクティブ化時には、アクティブなパイプラインが「非アクティブ化中の」状態になり、そして「非アクティブ」状態になります。 削除できるのは、非アクティブなパイプラインだけです。

ライブ パイプラインは、アクティブにした後もアクティブな状態を維持し、ソース (カメラ) からのライブ ビデオの処理を続けするように設計されています。 処理を停止するには、明示的な非アクティブ化コマンドが必要です。
パイプラインは、データが流れていない間もアクティブにすることができます (たとえば、入力ビデオ ソースがオフラインになった場合など)。 Azure サブスクリプションは、パイプラインがアクティブ状態である時に対して課金されます。

### <a name="batch-pipeline"></a>バッチパイプライン

パイプラインのライフサイクルを次の図に示します。

> [!div class="mx-imgBorder"]
> :::image type="content" source="./media/pipeline/batch-pipeline-lifecycle.svg" alt-text="バッチ パイプラインのライフサイクルの図。":::

パイプラインジョブが正常に作成されると、"処理中" 状態になります。 ジョブが正常に完了した場合は、"完了" 状態になります。失敗した場合は、"Failed" 状態になります。 または、パイプライン ジョブが "処理中" 状態である間に、キャンセル要求を発行できます。 その要求が成功すると、ジョブは 'Canceled' 状態になります。 Azure サブスクリプションは、パイプライン ジョブが正常に完了した場合にのみ請求されます。

トポロジのパラメーターに異なる値を指定することで、1 つのトポロジから複数のパイプラインを作成できます。 たとえば、異なるビデオ記録に対して同じトポロジを使用してパイプラインジョブを送信することができます。 トポロジは、すべてのパイプラインが削除されたときに削除できます。

## <a name="sources-processors-and-sinks"></a>ソース、プロセッサ、およびシンク

Video Analyzer では、次のノードを使用してパイプライントポロジを定義できます。

> [!NOTE]
> すべてのノードが、Video Analyzer edge モジュールとサービスの両方で使用できるわけではありません。 ノードの使用 [に関する規則に関するページを参照してください](pipeline.md#rules-on-the-use-of-nodes)。

### <a name="sources"></a>変換元

#### <a name="rtsp-source"></a>RTSP ソース

RTSP ソース ノードを使用すると、RTSP 対応カメラからメディアをキャプチャできます。詳細については、 [こちらを](quotas-limitations.md#supported-cameras) 参照してください。 RTSP ソース ノードでは、認証された接続を有効にするための資格情報と共に RTSP URL を指定する必要があります。

#### <a name="iot-hub-message-source"></a>IoT Hub メッセージのソース

他の [IoT Edge モジュール](../../iot-fundamentals/iot-glossary.md?view=iotedge-2020-11&preserve-view=true#iot-edge-device)と同様に、Azure Video Analyzer モジュールでは、[IoT Edge ハブ](../../iot-fundamentals/iot-glossary.md?view=iotedge-2020-11&preserve-view=true#iot-edge-hub)を介してメッセージを受信できます。 メッセージは、他のモジュール、またはエッジ デバイスで実行されているアプリ、またはクラウドから送信できます。 このようなメッセージは、ビデオ アナライザー モジュールの[名前付きの入力](../../iot-edge/module-composition.md?view=iotedge-2020-11&preserve-view=true#sink)に配信 (ルーティング) されます。 IoT Hub メッセージの送信元ノードを使用すると、このようなメッセージをパイプラインに取り込むことができます。 次に、メッセージをパイプラインで使用して、シグナル ゲートをアクティブ化できます (以下の「[signal gates ](#signal-gate-processor)」\(シグナル ゲート\)を参照)。

たとえば、ドアが開かれたときにメッセージを生成する IoT Edge モジュールがあります。 そのモジュールからのメッセージを IoT Edge ハブ にルーティングし、このハブから、パイプラインの IoT Hub メッセージ ソースにルーティングできます。 パイプライン内では、IoT Hub のメッセージ ソースからシグナル ゲート プロセッサにメッセージを渡すことができます。これにより、RTSP ソースからファイルへのビデオの記録が有効になります。

#### <a name="video-source"></a>ビデオ ソース

ビデオアナライザーで記録されたビデオコンテンツをソースとして使用できるようにします。 ノードでは、 [ビデオリソース](terminology.md#video)の名前を指定する必要があります。また、記録されたビデオを処理する開始時刻と終了時刻も指定する必要があります。

### <a name="processors"></a>[プロセッサ]

#### <a name="motion-detection-processor"></a>モーション検出プロセッサ

モーション検出プロセッサ ノードを使用すると、ライブ ビデオのモーションを検出できます。 受信したビデオ フレームを調べ、ビデオに動きがあるかどうかを判断します。 モーションが検出されると、ビデオ フレームをパイプラインの次のノードに渡し、イベントを出力します。 モーション検出プロセッサ ノードを (他のノードと組み合わせて) 使用すると、モーションが検出されたときに受信ビデオの記録をトリガーできます。

#### <a name="http-extension-processor"></a>HTTP 拡張プロセッサ

HTTP 拡張プロセッサ ノードを使用すると、独自の IoT Edge モジュールにパイプラインを接続できます。 このノードは、デコードされたビデオ フレームを入力として受け取り、そのようなフレームをモジュールによって公開される HTTP REST エンドポイントにリレーします。この場合、AI モデルを使用してフレームを分析し、推定結果を返すことができます。 また、このノードには、ビデオ フレームを HTTP エンドポイントにリレーする前に、スケールおよびエンコードするための組み込みのイメージ フォーマッタがあります。 スケーラーには、画像の縦横比を維持、埋め込み、または拡張するオプションがあります。 イメージ エンコーダーでは、JPEG、PNG、BMP、RAW 形式がサポートされています。 プロセッサの詳細については、[こちら](pipeline-extension.md#http-extension-processor)をご覧ください。

#### <a name="grpc-extension-processor"></a>gRPC 拡張プロセッサ

gRPC 拡張プロセッサ ノードは、デコードされたビデオ フレームを入力として受け取り、そのようなフレームをモジュールによって公開される [gRPC](terminology.md#grpc) エンドポイントにリレーします。 ノードでは、[共有メモリ](https://en.wikipedia.org/wiki/Shared_memory)を使用したデータの転送や、gRPC メッセージの本文へのフレームの直接埋め込みをサポートします。 また、HTTP 拡張機能と同様に、このノードには、ビデオ フレームを gRPC エンドポイントにリレーする前に、スケールおよびエンコードするための組み込みのイメージ フォーマッタがあります。 プロセッサの詳細については、[こちら](pipeline-extension.md#grpc-extension-processor)をご覧ください。

#### <a name="cognitive-services-extension-processor"></a>Cognitive Services 拡張機能のプロセッサ

Cognitive Services 拡張機能プロセッサ ノードを使用すると、パイプラインを [空間分析](https://azure.microsoft.com/services/cognitive-services/computer-vision/) IoT Edge モジュールに拡張できます。 このノードは、デコードされたビデオ フレームを入力として受け取り、公開される [gRPC](pipeline-extension.md#grpc-extension-processor) エンドポイントにそのようなフレームをリレーします。この場合、空間分析スキルを使用してフレームを分析し、推論結果を返すことができます。 プロセッサの詳細については、[こちら](pipeline-extension.md#cognitive-services-extension-processor)をご覧ください。

#### <a name="signal-gate-processor"></a>シグナル ゲート プロセッサ

シグナル ゲート プロセッサ ノードを使用すると、あるノードから別のノードにメディアを条件付きで転送できます。 シグナル ゲート プロセッサ ノードの直後に、ビデオ シンクまたはファイル シンクを配置する必要があります。 あるユース ケースでは、RTSP ソース ノードとビデオ シンク ノードの間にシグナル ゲート プロセッサ ノードを挿入し、モーション検出プロセッサ ノードの出力を使用してゲートをトリガーします。 このようなパイプラインでは、モーションが検出された場合にのみビデオが録画されます。 また、HTTP または gRPC 拡張機能ノードからの出力を使用して、モーション検出プロセッサノードではなくゲートをトリガーすることもできます。これにより、興味深いものが検出された場合にビデオを記録できるようになります。

#### <a name="object-tracker-processor"></a>オブジェクトトラッカーのプロセッサ

オブジェクト トラッカー プロセッサ ノードを使用すると、上流の HTTP または gRPC 拡張機能プロセッサノードで検出されたオブジェクトを追跡できます。 このノードは、すべてのフレーム内のオブジェクトを検出する必要があるが、エッジ デバイスがすべてのフレームに AI モデルを適用するために必要な計算機能を備えなていない場合に便利です。 10 フレームごとにコンピューター ビジョン モデルのみを実行できる場合、オブジェクト トラッカーはそのようなフレームの 1 つから結果を取得し、[光学フロー](https://en.wikipedia.org/wiki/Optical_flow)手法を使用して、モデルが次のフレームに再度適用されるまで、2 番目、3 番目,..., 9 番目のフレームの結果を生成できます。 このノードを使用する場合、計算のパワーと精度の間にはトレードオフがあります。 AI モデルが適用されるフレームが近いほど、精度は向上します。 ただし、これは AI モデルの適用頻度が高く、より高い計算パワーに変換されるという意味です。 オブジェクトが線を越えたときに検出するというのも、オブジェクト トラッカー プロセッサ ノードの一般的な使用の 1 つです。

#### <a name="line-crossing-processor"></a>ライン クロッシング プロセッサ

ライン クロッシング プロセッサ ノードを使用すると、自分で定義した線をオブジェクトが越えたときに検出できます。 さらに、(パイプラインがアクティブ化された時点から) 線を越えたオブジェクトの数も保持されます。 このノードは、オブジェクト トラッカー プロセッサ ノードの下流で使用する必要があります。

#### <a name="encoder-processor"></a>エンコーダー プロセッサ

録画されたビデオをダウンストリーム処理の目的の形式に変換するときに、ユーザーがエンコード プロパティを指定できるようにします。 たとえば [、4K](https://en.wikipedia.org/wiki/4K_resolution) 解像度用に構成されたカメラから記録されたビデオは、ファイルにエクスポートする前にフル [HD (1920x1080)](https://en.wikipedia.org/wiki/1080p) 解像度にサイズ変更する必要がある場合があります。

### <a name="sinks"></a>シンク

#### <a name="video-sink"></a>ビデオ シンク

ビデオ シンク ノードを使用すると、ビデオと関連するメタデータを Video Analyzer クラウド リソースに保存できます。 ビデオは、継続的またはまばらに (イベントに基づいて) 記録できます。 ビデオ シンク ノードは、クラウドへの接続が失われた場合にエッジ デバイスにビデオをキャッシュし、接続が復元されるとアップロードを再開できます。 このノードのプロパティの使用方法の詳細については、[継続的なビデオ記録](continuous-video-recording.md)に関する記事も参照してください。

セキュリティ上の理由から、特定の Video **Analyzer** エッジ モジュール インスタンスは、新しいビデオ エントリ、または以前に同じモジュールによって記録されたビデオ エントリにのみコンテンツを記録できます。 同じエッジ モジュール インスタンスによって作成されていない既存のビデオにコンテンツを記録しようとすると、記録に失敗します。

#### <a name="file-sink"></a>ファイル シンク

ファイル シンク ノードを使用すると、エッジ デバイスのローカル ファイル システム上の場所にビデオを書き込むことができます。 パイプラインには 1 つのファイル シンク ノードしか存在できません。シグナル ゲート プロセッサ ノードのダウンストリームである必要があります。 これにより、出力ファイルの長さが、シグナル ゲート プロセッサのノード プロパティで指定された値に制限されます。 エッジ デバイスのディスク領域が不足しないようにするために、Video Analyzer エッジ モジュールがデータをキャッシュするために使用できる最大サイズを設定することもできます。

キャッシュがいっぱいになると、Video Analyzer エッジ モジュールによって最も古いデータの削除が開始され、新しいデータで置き換えられます。

#### <a name="iot-hub-message-sink"></a>IoT Hub メッセージ シンク

IoT Hub メッセージ シンク ノードを使用すると IoT Edge ハブにイベントを発行できます。 IoT Edge ハブは、エッジ デバイス上の他のモジュールやアプリに、またはクラウド内の IoT Hub にデータをルーティングするよう構成できます (配置マニフェストで指定されているルートごと)。 IoT Hub メッセージ シンク ノードは、モーション検出プロセッサ ノードなどのアップストリーム プロセッサ ノードから、または HTTP 拡張プロセッサ ノードを介して外部の推論サービスからイベントを受け取ることができます。

## <a name="rules-on-the-use-of-nodes"></a>ノードの使用に関する規則

次の表では、ライブ トポロジとバッチ トポロジ、および Video Analyzer エッジ モジュールとサービスでノードが許可されている現在の規則について説明します。

| ノード名                              | トポロジの種類  |  デプロイ       |
| -------------------------------------- | ---------------|-------------------|
| RTSP ソース                            | ライブ           |  エッジとクラウド   |
| IoT Hub メッセージ のソース                 | ライブ           |  Edge             |
| ビデオ ソース                           | Batch          |  クラウド            |
| モーション検出プロセッサ             | ライブ           |  Edge             |
| HTTP 拡張プロセッサ               | ライブ           |  Edge             |
| gRPC 拡張プロセッサ               | ライブ           |  Edge             |
| Cognitive Services 拡張プロセッサ | ライブ           |  Edge             |
| シグナル ゲート プロセッサ                  | ライブ           |  Edge             |
| オブジェクトトラッカーのプロセッサ               | ライブ           |  Edge             |
| ライン クロッシング プロセッサ                | ライブ           |  Edge             |
| エンコーダー プロセッサ                      | Batch          |  クラウド            |
| ビデオ シンク                             | ライブとバッチ |  エッジとクラウド   |
| ファイル シンク                              | ライブ           |  Edge             |
| IoT Hub メッセージ シンク                   | ライブ           |  Edge             |

パイプライン内でさまざまなノードを使用する上で適用される他の規則については、「[limitations on pipelines](quotas-limitations.md#limitations-on-pipeline-topologies)」\ (パイプラインに関する制限\) を参照してください。

## <a name="scenarios"></a>シナリオ

上記で定義したソース、プロセッサ、およびシンクを組み合わせて使用することで、ライブ ビデオの分析に関連するさまざまなシナリオでパイプラインを構築できます。 シナリオの例を次に示します。

* [継続的なビデオ記録](continuous-video-recording.md) 
* [イベントベースのビデオ記録](event-based-video-recording-concept.md) 
* [ビデオ記録を行わないビデオ解析](analyze-live-video-without-recording.md) 

## <a name="next-steps"></a>次の手順

ライブビデオ フィードでモーション検出を実行する方法については、「[Quickstart: Get started – Azure Video Analyzer](get-started-detect-motion-emit-events.md)」\ (クイックスタート: はじめに-Azure Video Analyzer\) を参照してください。
