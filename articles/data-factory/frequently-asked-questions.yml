### YamlMime:FAQ
metadata:
  title: 'Azure Data Factory: よく寄せられる質問 '
  description: Azure Data Factory についてよく寄せられる質問とその回答を紹介します。
  author: ssabat
  ms.author: susabat
  ms.service: data-factory
  ms.subservice: 
  ms.topic: conceptual
  ms.date: 07/23/2021
  ms.openlocfilehash: 792d61cf017fc098db92b0ac1ec4670a08456069
  ms.sourcegitcommit: f6e2ea5571e35b9ed3a79a22485eba4d20ae36cc
  ms.translationtype: HT
  ms.contentlocale: ja-JP
  ms.lasthandoff: 09/24/2021
  ms.locfileid: "128571381"
title: Azure Data Factory FAQ
summary: "[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]\n\nこの記事では、Azure Data Factory に関してよく寄せられる質問に対する回答を示します。  \n"
sections:
- name: 無視
  questions:
  - question: >
      Azure Data Factory とは何ですか。
    answer: "Data Factory は、データの移動や変換を自動化するフル マネージドのクラウドベースのデータ統合 ETL サービスです。 原材料を機械で加工して最終製品を作成する工場と同じように、Azure Data Factory は生データを収集してすぐに使用できる情報に変換する既存のサービスを調整します。 \n\nAzure Data Factory を使用すると、データ駆動型のワークフローを作成して、オンプレミスとクラウドのデータ ストア間でデータを移動できます。 また、Data Flow を使用してデータを処理したり変換したりすることができます。 ADF は、外部のコンピューティング エンジンもサポートしており、Azure HDInsight、Azure Databricks、SQL Server Integration Services (SSIS) 統合ランタイムなどのコンピューティング サービスを使用して、手動でコーディングされた変換を実行できます。\n\nData Factory を使用すると、Azure ベースのクラウド サービスか、SSIS、SQL Server、Oracle などの独自のセルフホステッド コンピューティング環境でデータ処理を実行できます。 必要なアクションを実行するパイプラインを作成した後で、それを毎時、毎日、毎週など、定期的に実行するようにスケジュールを設定できます (時間ウィンドウ スケジューリング)。また、イベントの発生によってパイプラインをトリガーすることもできます。 詳細については、[Azure Data Factory の概要](introduction.md)に関するページをご覧ください。\n"
  - question: >
      コンプライアンスとセキュリティに関する考慮事項
    answer: "Azure Data Factory は、_SOC 1、2、3_、_HIPAA BAA_、_HITRUST_ など、幅広いコンプライアンス認定で認定されています。 増え続けているすべての認定の一覧については、[こちら](data-movement-security-considerations.md)を参照してください。 監査レポートとコンプライアンス認定のデジタル コピーについては、[サービス トラスト センター](https://servicetrust.microsoft.com/)を参照してください\n\n### <a name=\"control-flows-and-scale\"></a>フローとスケールの制御\n\n最新のデータ ウェアハウスの多様な統合フローとパターンをサポートするため、Data Factory では、柔軟なデータ パイプライン モデルを利用できます。 これには、条件付き実行やデータ パイプラインの分岐、また、フロー内やフロー間でパラメーターを明示的に渡す機能など、制御フローの全面的なプログラミング パラダイムが必要です。 また、制御フローには、コピー アクティビティを介した大規模なデータ移動など、外部の実行エンジンへのアクティビティのディスパッチやデータ フローの機能を通じたデータの変換が含まれます。\n\nData Factory はデータ統合に必要な任意のフロー スタイルをモデル化し、それをオンデマンドで、またはスケジュールどおりに繰り返し送信することができます。 次の一般的なフローがこのモデルで有効になります。\n\n- 制御フロー:\n    - アクティビティは、パイプライン内で連結して 1 つのシーケンスにすることができます。\n    - アクティビティは、パイプライン内で分岐できます。\n    - パラメーター:\n        * パイプライン レベルでパラメーターを定義でき、オンデマンドで、またはトリガーからパイプラインを呼び出す際に引数を渡すことができます。\n        * アクティビティは、パイプラインに渡される引数を使用できます。\n    - カスタム状態の受け渡し:\n        * 状態などのアクティビティ出力は、パイプライン内の後続のアクティビティで使用できます。\n    - ループ コンテナー:\n        * foreach アクティビティは、ループ内の指定されたアクティビティのコレクションに対して反復されます。 \n- トリガー ベースのフロー:\n    - パイプラインは、オンデマンドで、時刻によって、またはイベント グリッドのトピックによる駆動に応じてトリガーすることができます。\n- 差分フロー:\n    - オンプレミスまたはクラウド内のリレーショナル ストアからディメンションまたは参照テーブルを移動する間の差分コピーの最大値をパラメーターを使用して定義し、レイク内にデータを読み込みことができます。\n\n詳細については、「[チュートリアル:制御フロー](tutorial-control-flow.md)に関するページを参照してください。\n\n### <a name=\"data-transformed-at-scale-with-code-free-pipelines\"></a>コード不要のパイプラインを使用して大規模に変換されるデータ\n\nこの新しいブラウザーベースのツール エクスペリエンスは、最新のインタラクティブな Web ベースのエクスペリエンスにより、コード不要のパイプラインのオーサリングとデプロイを提供します。\n\nビジュアル データの開発者やデータ エンジニアにとっては、Data Factory Web UI がパイプラインのビルドに使用するコード不要のデザイン環境です。 Visual Studio Online Git と完全に統合されており、CI/CD およびデバッグ オプション付きの反復開発との統合を提供します。\n\n### <a name=\"rich-cross-platform-sdks-for-advanced-users\"></a>詳しい知識のあるユーザー向けの豊富なクロス プラットフォーム SDK\n\nData Factory V2 の充実した SDK セットを使用すると、使い慣れた IDE でパイプラインを作成、管理、および監視できます。たとえば、次のものを使用できます。\n\n* Python SDK\n* PowerShell CLI\n* C# SDK\n\nユーザーは、文書化されている REST API を Data Factory V2 に対するインターフェイスとして使用することもできます。\n\n### <a name=\"iterative-development-and-debugging-by-using-visual-tools\"></a>ビジュアル ツールを使用した反復開発とデバッグ\n\nAzure Data Factory ビジュアル ツールを使用すると、反復開発とデバッグを行うことができます。 パイプラインを作成し、パイプライン キャンバスの **デバッグ** 機能を使用して、1 行もコードを記述せずにテストを実行できます。 テストの実行結果は、パイプライン キャンバスの **[出力]** ウィンドウに表示されます。 テストの実行が成功したら、パイプラインにさらにアクティビティを追加し、反復的な方法でデバッグを続行できます。 進行中になったテストをキャンセルすることもできます。\n\n**[デバッグ]** を選択する前にデータ ファクトリ サービスの変更を発行する必要はありません。 これは、データ ファクトリのワークフローを更新する前に、新しい追加や変更が開発環境、テスト環境、運用環境で期待どおりに動作することを確認する場合に便利です。\n\n### <a name=\"ability-to-deploy-ssis-packages-to-azure\"></a>SSIS パッケージを Azure にデプロイする機能\n\nSSIS ワークロードを移動する場合は、データ ファクトリを作成し、Azure-SSIS 統合ランタイムをプロビジョニングできます。 Azure-SSIS 統合ランタイムは、クラウドでの SSIS パッケージの実行専用の、Azure VM (ノード) のフル マネージドのクラスターです。 詳しい手順については、「[SSIS パッケージを Azure にデプロイする](./tutorial-deploy-ssis-packages-azure.md)」チュートリアルを参照してください。 \n\n### <a name=\"sdks\"></a>SDK\n\nプログラマティック インターフェイスを必要とする詳しい知識のあるユーザー向けに、Data Factory では、使い慣れた IDE を使用してパイプラインを作成、管理、監視するために使用できる豊富な SDK セットが用意されています。 .NET、PowerShell、Python、REST などの言語がサポートされています。\n\n### <a name=\"monitoring\"></a>監視\n\nデータ ファクトリは、PowerShell、SDK、またはブラウザー ユーザー インターフェイス内のビジュアル モニタリング ツールで監視できます。 オンデマンド、トリガー ベース、およびクロック駆動のカスタム フローで効果的かつ効率的に監視および管理できます。 既存タスクをキャンセルする、一目でエラーを確認する、ドリルダウンして詳細なエラー メッセージを取得する、問題をデバッグするなど、コンテキストを切り替えたり、画面を前後に移動したりすることなく、すべて 1 つのウィンドウで行うことができます。\n\n### <a name=\"new-features-for-ssis-in-data-factory\"></a>Data Factory の SSIS の新機能\n\n2017 年のパブリック プレビュー リリースより、Data Factory は SSIS に次の機能を追加しています。\n\n-    プロジェクトやパッケージの SSIS データベース (SSISDB) をホストする、Azure SQL Database の次の 3 つの追加構成およびバリアントのサポート。\n-    仮想ネットワーク サービス エンドポイントを使用する SQL Database\n-    SQL Managed Instance\n-    エラスティック プール\n-    将来廃止されるクラシック仮想ネットワーク上の Azure Resource Manager 仮想ネットワークに対するサポート。これにより、仮想ネットワーク サービス エンドポイント/MI/オンプレミス データ アクセスを使用する SQL Database 用に構成された仮想ネットワークに対して、Azure-SSIS 統合ランタイムを挿入したり、参加させたりできます。 詳細については、「[Azure-SSIS 統合ランタイムを仮想ネットワークに参加させる](join-azure-ssis-integration-runtime-virtual-network.md)」も参照してください。\n-    SSISDB に接続する際の Azure Active Directory (Azure AD) 認証および SQL 認証のサポート。これにより、Azure リソース用の Data Factory マネージド ID で Azure AD 認証を行うことができます。\n-    既存の SQL Server ライセンスの利用のサポート。Azure ハイブリッド特典オプションから実質的なコスト削減を得られます。\n-    Azure-SSIS 統合ランタイムのエンタープライズ エディションのサポート。これにより、高度な機能やプレミアム機能、追加のコンポーネントや拡張機能をインストールするカスタム セットアップ インターフェイス、およびパートナーのエコシステムを使用できます。 詳細については、「[Enterprise Edition, Custom Setup, and 3rd Party Extensibility for SSIS in ADF (ADF の SSIS 用のエンタープライズ エディション、カスタム セットアップ、およびサード パーティの拡張性)](https://blogs.msdn.microsoft.com/ssis/2018/04/27/enterprise-edition-custom-setup-and-3rd-party-extensibility-for-ssis-in-adf/)」を参照してください。 \n-    Data Factory と SSIS のより深い統合。これにより、Data Factory パイプラインのファースト クラスの SSIS パッケージ実行アクティビティを呼び出し/トリガーし、SSMS でそれらのスケジュールを設定できます。 詳細については、「[Modernize and extend your ETL/ELT workflows with SSIS activities in ADF pipelines (ADF パイプラインでの SSIS アクティビティを含む ETL/ELT ワークフローの最新化と拡張)](https://blogs.msdn.microsoft.com/ssis/2018/05/23/modernize-and-extend-your-etlelt-workflows-with-ssis-activities-in-adf-pipelines/)」を参照してください。\n"
  - question: >
      統合ランタイムについて
    answer: >
      統合ランタイムは、さまざまなネットワーク環境間で以下のデータ統合機能を提供するために Azure Data Factory で使用されるコンピューティング インフラストラクチャです。


      - **データ移動**:データ移動では、統合ランタイムは移動元と移動先のデータ ストア間でデータを移動しながら、組み込みのコネクタ、形式変換、列マッピング、および高性能でスケーラブルなデータ転送のサポートを提供します。

      - **データ フロー**: データ フローについては、マネージド Azure コンピューティング環境で [データ フロー](./concepts-data-flow-overview.md)を実行します。

      - **アクティビティの送信**: 変換では、統合ランタイムは、SSIS パッケージをネイティブに実行する機能を提供します。

      - **SSIS パッケージの実行**: 統合ランタイムは、マネージド Azure コンピューティング環境で SSIS パッケージをネイティブに実行します。 統合ランタイムでは、Azure HDInsight、Azure Machine Learning、SQL Database、SQL Server などのさまざまなコンピューティング サービス上で実行される変換アクティビティのディスパッチや監視もサポートされています。


      データの移動と変換に必要な 1 つ以上の統合ランタイム インスタンスをデプロイできます。 統合ランタイムは、Azure パブリック ネットワークまたはプライベート ネットワーク (オンプレミス、Azure Virtual Network、またはアマゾン ウェブ サービス Virtual Private Cloud (VPC)) 上で実行できます。

      Data Factory で、アクティビティは、実行されるアクションを定義します。 リンクされたサービスは、ターゲットのデータ ストアやコンピューティング サービスを定義します。

      統合ランタイムは、アクティビティとリンクされたサービスとを橋渡しします。 リンクされたサービスまたはアクティビティによって参照され、アクティビティが実行されたりディスパッチされたりするコンピューティング環境を提供します。 これにより、できるだけターゲットのデータ ストアやコンピューティング サービスに近いリージョンでアクティビティを実行して効率を最大化できる一方、セキュリティとコンプライアンスの必要も満たせます。


      統合ランタイムは、管理ハブおよびそれらを参照するすべてのアクティビティ、データセット、またはデータ フローを使用して、Azure Data Factory UX で作成できます。

      詳細については、「[Azure Data Factory の統合ランタイム](./concepts-integration-runtime.md)」を参照してください。
  - question: >
      Integration Runtime 数の制限
    answer: >
      1 つのデータ ファクトリに含めることができる統合ランタイム インスタンスの数に厳密な制限はありません。 ただし、SSIS パッケージの実行について、1 サブスクリプションにつき統合ランタイムが使用できる VM コア数には制限があります。 詳細については、「[Data Factory の制限](../azure-resource-manager/management/azure-subscription-service-limits.md#data-factory-limits)」を参照してください。
  - question: >
      Azure Data Factory の最上位の概念とは
    answer: "1 つの Azure サブスクリプションで 1 つ以上の Azure Data Factory インスタンス (データ ファクトリ) を利用できます。 Azure Data Factory には、プラットフォームとして連携する 4 つの主要コンポーネントが含まれます。このプラットフォームを基盤とし、データ移動とデータ変換のステップを含んだデータ主導型のワークフローを作成することができます。\n\n### <a name=\"pipelines\"></a>パイプライン\n\nデータ ファクトリは、1 つまたは複数のパイプラインを持つことができます。 パイプラインは、1 つの作業単位を実行するための複数のアクティビティから成る論理的なグループです。 パイプライン内のアクティビティがまとまって 1 つのタスクを実行します。 たとえば、Azure BLOB からデータを取り込み、HDInsight クラスターで Hive クエリを実行してデータを分割するアクティビティのグループをパイプラインに含めることができます。 パイプラインには、アクティビティをセットとして管理でき、個別に管理せずに済むというメリットがあります。 パイプライン内のアクティビティは、連鎖して順次処理することも、独立して並行処理することもできます。\n\n### <a name=\"data-flows\"></a>データ フロー\n\nデータ フローは、バックエンドの Spark サービスで大規模なデータを変換するオブジェクトです。Data Factory で視覚的に作成します。 プログラミングや Spark 内部についての知識は必要ありません。 グラフ (マッピング) またはスプレッドシート (Power Query アクティビティ) を使用してデータ変換の意図を設計するだけです。\n\n ### <a name=\"activities\"></a>Activities\n\nアクティビティは、パイプライン内の処理ステップを表します。 たとえば、コピー アクティビティを使用して、データ ストア間でデータをコピーできます。 同様に、Azure HDInsight クラスターに対して Hive クエリを実行する Hive アクティビティを使用して、データを変換または分析できます。 Data Factory では、データ移動アクティビティ、データ変換アクティビティ、制御アクティビティの 3 種類のアクティビティがサポートされています。\n\n### <a name=\"datasets\"></a>データセット\n\nデータセットは、データ ストア内のデータ構造を表しています。アクティビティ内でデータを入力または出力として使用したい場合は、そのデータをポイントまたは参照するだけで済みます。 \n\n### <a name=\"linked-services\"></a>リンクされたサービス\n\nリンクされたサービスは、接続文字列によく似ており、Data Factory が外部リソースに接続するために必要な接続情報を定義します。 リンクされたサービスはデータ ソースへの接続を定義するもので、データセットはデータの構造を表すもの、と捉えることができます。 たとえば、Azure Storage のリンクされたサービスでは、Azure Storage アカウントに接続するための接続文字列を指定します。 また、データを格納する BLOB コンテナーやフォルダーは、Azure BLOB データセットで指定します。\n\nData Factory では、リンクされたサービスは 2 つの目的に使用されます。\n\n- SQL Server インスタンス、Oracle データベース インスタンス、ファイル共有、Azure Blob Storage アカウント、その他の \"*データ ストア*\" を表すため。 サポートされるデータ ストアの一覧については、「[Azure Data Factory のコピー アクティビティ](copy-activity-overview.md)」を参照してください。\n- アクティビティの実行をホストできる *コンピューティング リソース* を表すため。 たとえば、HDInsight Hive アクティビティは HDInsight Hadoop クラスターで実行されます。 変換アクティビティとサポートされているコンピューティング環境の一覧については、「[Azure Data Factory でデータを変換する](transform-data.md)」を参照してください。\n\n### <a name=\"triggers\"></a>トリガー\n\nトリガーは、パイプラインの実行をいつ開始するかを決定する処理単位を表します。 さまざまな種類のイベントに合わせて、さまざまな種類のトリガーがあります。 \n\n### <a name=\"pipeline-runs\"></a>パイプライン実行\n\nパイプライン実行は、パイプラインを実行するインスタンスです。 通常は、パイプラインで定義されたパラメーターに引数を渡すことで、パイプライン実行をインスタンス化します。 引数は、手動で渡すことも、トリガー定義内で渡すこともできます。\n\n### <a name=\"parameters\"></a>パラメーター\n\nパラメーターは、読み取り専用構成のキーと値のペアです。 パラメーターはパイプラインで定義し、定義されたパラメーターの引数を実行時に実行コンテキストから渡します。 実行コンテキストは、トリガーによって、または手動で実行するパイプラインから作成されます。 パイプライン内のアクティビティは、パラメーターの値を使用します。\n\nデータセットとは、厳密に型指定されたパラメーターと、再利用または参照可能なエンティティのことです。 アクティビティは、データセットを参照でき、データセットの定義で定義されたプロパティを使用できます。\n\nリンクされたサービスも厳密に型指定されたパラメーターであり、これにはデータ ストアかコンピューティング環境への接続情報が入ります。 これも、再利用または参照可能なエンティティです。\n\n### <a name=\"control-flows\"></a>制御フロー\n\n制御フローは、パイプライン アクティビティを調整します。これには、シーケンス内のアクティビティの連鎖、分岐、パイプライン レベルで定義するパラメーター、オンデマンドかトリガーからパイプラインを起動した際に渡す引数が含まれます。 さらに、カスタム状態の受け渡しや、ループ コンテナー (つまり foreach 反復子) も制御フローに含まれます。\n\n\nData Factory の概念について詳しくは、次の記事をご覧ください。\n\n- [データセットとリンクされたサービス](concepts-datasets-linked-services.md)\n- [パイプラインとアクティビティ](concepts-pipelines-activities.md)\n- [Integration Runtime](concepts-integration-runtime.md)\n"
  - question: >
      Data Factory の価格モデル
    answer: >
      Azure Data Factory の価格の詳細については、[Data Factory の価格の詳細](https://azure.microsoft.com/pricing/details/data-factory/)に関するページを参照してください。
  - question: >
      Data Factory の最新情報を入手するにはどうすればよいですか。
    answer: >
      Azure Data Factory の最新情報を入手するには、次のサイトを参照してください。


      - [ブログ](https://azure.microsoft.com/blog/tag/azure-data-factory/)

      - [ドキュメントのホーム ページ](./index.yml)

      - [製品のホーム ページ](https://azure.microsoft.com/services/data-factory/)
  - question: >
      技術的な詳細情報
    answer: "### <a name=\"how-can-i-schedule-a-pipeline\"></a>パイプラインのスケジュールを設定するにはどうすればよいですか。\n\nパイプラインのスケジュールを設定するには、スケジューラのトリガーか時間ウィンドウ トリガーを使用できます。 トリガーでは、実時間カレンダー スケジュールが使用されます。これにより、定期的に、またはカレンダーベースの周期パターン (毎週月曜日午後 6 時 00 分、毎週木曜日午後 9 時 00 分など) でパイプラインをスケジュールできます。 詳細については、[パイプラインの実行とトリガー](concepts-pipeline-execution-triggers.md)に関するページを参照してください。\n\n### <a name=\"can-i-pass-parameters-to-a-pipeline-run\"></a>実行されるパイプラインにパラメーターを渡すことはできますか。\n\nはい。パラメーターは Data Factory でファースト クラスの最上位の概念です。 パイプライン レベルでパラメーターを定義し、パイプライン実行をオンデマンドで実行するときまたはトリガーを使用して実行するときに、引数を渡すことができます。  \n\n### <a name=\"can-i-define-default-values-for-the-pipeline-parameters\"></a>パイプライン パラメーターの既定値は定義できますか?\n\nはい。 パイプラインではパラメーターの既定値を定義できます。\n\n### <a name=\"can-an-activity-in-a-pipeline-consume-arguments-that-are-passed-to-a-pipeline-run\"></a>パイプラインのアクティビティはパイプライン実行に渡される引数を使用できますか?\n\nはい。 パイプライン内の各アクティビティは、パイプラインに渡されて `@parameter` コンストラクトで実行されるパラメーター値を使用できます。 \n\n### <a name=\"can-an-activity-output-property-be-consumed-in-another-activity\"></a>アクティビティの出力プロパティは別のアクティビティで使用できますか?\n\nはい。 アクティビティの出力は、`@activity` コンストラクトを使用して後続のアクティビティで使用できます。\n \n### <a name=\"how-do-i-gracefully-handle-null-values-in-an-activity-output\"></a>アクティビティの出力で null 値を適切に処理するにはどうすればよいですか?\n\n式で `@coalesce` コンストラクトを使用すると、null 値を適切に処理できます。\n"
  - question: >
      データ フローのマッピング
    answer: "### <a name=\"i-need-help-troubleshooting-my-data-flow-logic-what-info-do-i-need-to-provide-to-get-help\"></a>データ フロー ロジックのトラブルシューティングのサポートが必要です。 サポートを受けるには、どのような情報を用意すればよいですか?\n\nMicrosoft では、データ フローに関するサポートやトラブルシューティングを行っています。ADF パイプライン サポート ファイルをご用意ください。\nこの Zip ファイルには、データ フロー グラフの分離コード スクリプトが含まれています。 ADF UI からパイプラインの横にある **[...]** をクリックし、**[Download support files]\\(サポート ファイルのダウンロード\\)** をクリックします。\n\n### <a name=\"how-do-i-access-data-by-using-the-other-90-dataset-types-in-data-factory\"></a>Data Factory で他の 90 個のデータセット型を使用してデータにアクセスする方法はありますか?\n\n現在、マッピング データ フロー機能では、ネイティブのソースおよびシンクとして、Azure SQL Database、Azure Synapse Analytics のほか、Azure Blob Storage または Azure Data Lake Storage Gen2 からの区切りテキスト ファイル、および BLOB ストレージまたは Data Lake Storage Gen2 からの Parquet ファイルが許可されています。 \n\nコピー アクティビティを使用して、データを他の任意のコネクタから段階的に送り、Data Flow のアクティビティを実行してステージングの後にデータを変換します。 たとえば、まずパイプラインを BLOB ストレージにコピーし、次に Data Flow のアクティビティでソースのデータセットを使用して、データを変換します。\n\n### <a name=\"is-the-self-hosted-integration-runtime-available-for-data-flows\"></a>データ フローにセルフホステッド統合ランタイムは使用できますか?\n\nセルフホステッド IR は ADF パイプライン コンストラクトであり、コピー アクティビティと共に使用して、オンプレミスまたは VM ベースのデータ ソースおよびシンクに対してデータを取得または移動することができます。 セルフホステッド IR に使用する仮想マシンは、保護されたデータ ストアと同じ VNET 内に配置して、ADF からそれらのデータ ストアにアクセスすることもできます。 データ フローを使用すると、代わりにマネージド VNET で Azure IR を使用して、これらと同じ最終結果を得ることができます。\n\n### <a name=\"does-the-data-flow-compute-engine-serve-multiple-tenants\"></a>データ フロー コンピューティング エンジンは複数のテナントにサービスを提供しますか?\n\nクラスターが共有されることはありません。 運用環境で実行されるジョブごとの分離が保証されています。 デバッグ シナリオの場合、1 人のユーザーが 1 つのクラスターを取得し、そのユーザーによって開始されるそのクラスターにすべてのデバッグが送られます。\n\n### <a name=\"is-there-a-way-to-write-attributes-in-cosmos-db-in-the-same-order-as-specified-in-the-sink-in-adf-data-flow\"></a>属性を ADF データ フローのシンクで指定されているのと同じ順序で Cosmos DB に書き込む方法はありますか?    \n\nCosmos DB では、各ドキュメントの基になる形式は、順序付けられていない名前と値のペアである JSON オブジェクトであるため、順序を予約することはできません。 \n\n### <a name=\"why-a-user-is-unable-to-use-data-preview-in-the-data-flows\"></a>ユーザーがデータ フローでデータ プレビューを使用できないのはなぜですか? \n\nカスタム ロールのアクセス許可を確認してください。 データフロー データのプレビューには複数のアクションが関係しています。 まず、ブラウザーでデバッグ中にネットワーク トラフィックをチェックします。 すべてのアクションに従ってください。詳細については、[リソース プロバイダー](../role-based-access-control/resource-provider-operations.md#microsoftdatafactory)に関するページを参照してください。\n\n### <a name=\"in-adf-can-i-calculate-value-for-a-new-column-from-existing-column-from-mapping\"></a>ADF では、マッピングからの既存の列から新しい列の値を計算できますか?   \n\nマッピング データ フローで派生変換を使用して、必要なロジックで新しい列を作成できます。 派生列を作成するときは、新しい列を生成するか、既存の列を更新することができます。 [列] ボックスに、作成する列を入力します。 スキーマ内の既存の列を上書きするには、列ドロップダウンを使用できます。 派生列の式を作成するには、 [式の入力] ボックスをクリックします。 式の入力を開始するか、式ビルダーを開いてロジックを作成することができます。\n\n### <a name=\"why-mapping-data-flow-preview-failing-with-gateway-timeout\"></a>マッピング データ フローのプレビューがゲートウェイのタイムアウトで失敗するのはなぜですか? \n\nより大きなクラスターを使用し、デバッグ出力のサイズを小さくするために、デバッグ設定の行制限を利用して値を小さくしてみてください。\n\n### <a name=\"how-to-parameterize-column-name-in-dataflow\"></a>データフローの列名をパラメーター化する方法は?\n\n列名は、他のプロパティと同様にパラメーター化できます。 派生列の場合と同様に、お客様は、 **$ColumnNameParam = toString(byName($myColumnNameParamInData))** を使用できます。 これらのパラメーターは、パイプラインの実行からデータ フローに渡すことができます。\n\n### <a name=\"the-data-flow-advisory-about-ttl-and-costs\"></a>TTL とコストに関するデータ フローのアドバイザリ\n\nこのトラブルシューティング ドキュメントは、[マッピング データ フローのパフォーマンスとチューニングのガイド - Time to live](../data-factory/concepts-integration-runtime-performance.md#time-to-live) に関する問題の解決に役立つ場合があります。\n"
  - question: >
      Power Query データ ラングリング
    answer: "### <a name=\"what-are-the-supported-regions-for-data-wrangling\"></a>データ ラングリングでサポートされているリージョンを教えてください。\n\nデータ ファクトリは、次の[リージョン](https://azure.microsoft.com/global-infrastructure/services/?products=data-factory)でご利用いただけます。\nPower Query 機能は、すべてのデータ フロー リージョンで使用できます。 ご使用のリージョンでこの機能を利用できない場合は、サポートにお問い合わせください。\n               \n### <a name=\"what-is-the-difference-between-mapping-data-flow-and-power-query-actvity-data-wrangling\"></a>マッピング データ フローと Power Query アクティビティ (データ ラングリング) にはどのような違いがありますか。\n\nデータ フローのマッピングには、コーディングなしでデータを大規模に変換する機能が備わっています。 データ フロー キャンバスで一連の変換を構築することで、データ変換ジョブを設計できます。 任意の数のソースの変換から始め、その後にデータ変換手順を実行します。 シンクを使ってデータ フローを完了し、結果を宛先に書き込みます。 データ フローのマッピングは、シンクとソース内の既知のスキーマと不明なスキーマの両方でのデータのマップや変換に最適です。\n\nPower Query データ ラングリングを使うと、Spark の実行を通じて Power Query Online マッシュアップ エディターを大規模に使用して、アジャイルなデータ準備と探索を行うことができます。 データ レイクの使用が増加するなか、単にデータセットを探索したり、レイクにデータセットを作成したりしたい場合があります。 既知のターゲットにマッピングするのでなければ、\n\n### <a name=\"supported-sql-types\"></a>サポートされている SQL の型\n\nPower Query データ ラングリングでは、SQL で次のデータ型がサポートされています。 サポートされていないデータ型を使用すると、検証エラーが表示されます。\n\n* short\n* double\n* real\n* float\n* char\n* nchar\n* varchar\n* nvarchar\n* 整数 (integer)\n* INT\n* bit\n* boolean\n* smallint\n* tinyint\n* bigint\n* long\n* text\n* date\n* DATETIME\n* datetime2\n* smalldatetime\n* timestamp\n* UNIQUEIDENTIFIER\n* xml\n"
additionalContent: "\n## <a name=\"next-steps\"></a>次のステップ\n\nデータ ファクトリを作成する手順については、次のチュートリアルをご覧ください。\n        \n- 「[クイックスタート: データ ファクトリを作成する](quickstart-create-data-factory-dot-net.md)\n- [チュートリアル:クラウド内のデータをコピーする](tutorial-copy-data-dot-net.md)"
