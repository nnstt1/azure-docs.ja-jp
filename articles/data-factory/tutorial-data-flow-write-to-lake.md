---
title: データ フローを使用したデータ レイクへのファイル書き込みのベスト プラクティス
description: このチュートリアルは、データ フローを使用したデータ レイクへのファイル書き込みのベスト プラクティスを提供します
author: kromerm
ms.author: makromer
ms.service: data-factory
ms.subservice: data-flows
ms.topic: conceptual
ms.custom: seo-lt-2021
ms.date: 06/04/2021
ms.openlocfilehash: 02e67e6521e1f5fa3c29375a15953557613f7d7d
ms.sourcegitcommit: 0770a7d91278043a83ccc597af25934854605e8b
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 09/13/2021
ms.locfileid: "124763692"
---
# <a name="best-practices-for-writing-to-files-to-data-lake-with-data-flows"></a>データ フローを使用したデータ レイクへのファイル書き込みのベスト プラクティス

[!INCLUDE[appliesto-adf-asa-md](includes/appliesto-adf-asa-md.md)]

Azure Data Factory を初めて使用する場合は、「[Azure Data Factory の概要](introduction.md)」を参照してください。

このチュートリアルでは、データ フローを使用して ADLS Gen2 または Azure Blob Storage にファイルを書き込むときに適用できるベスト プラクティスについて説明します。 Parquet ファイルを読み取り、その結果をフォルダーに格納するには、Azure Blob Storage アカウントまたは Azure Data Lake Store Gen2 アカウントにアクセスする必要があります。

## <a name="prerequisites"></a>前提条件
* **Azure サブスクリプション**。 Azure サブスクリプションをお持ちでない場合は、開始する前に[無料の Azure アカウント](https://azure.microsoft.com/free/)を作成してください。
* **Azure ストレージ アカウント**。 ADLS ストレージを、*ソース* と *シンク* のデータ ストアとして使用します。 ストレージ アカウントがない場合の作成手順については、[Azure のストレージ アカウントの作成](../storage/common/storage-account-create.md)に関するページを参照してください。

このチュートリアルのステップでは、次が前提となっています 

## <a name="create-a-data-factory"></a>Data Factory の作成

この手順では、データ ファクトリを作成し、Data Factory UX を開いて、データ ファクトリにパイプラインを作成します。

1. **Microsoft Edge** または **Google Chrome** を開きます。 現在、Data Factory の UI がサポートされる Web ブラウザーは Microsoft Edge と Google Chrome だけです。
1. 左側のメニューで、 **[リソースの作成]**  >  **[統合]**  >  **[Data Factory]** を選択します。
1. **[新しいデータ ファクトリ]** ページで、 **[名前]** に「**ADFTutorialDataFactory**」と入力します。
1. データ ファクトリを作成する Azure **サブスクリプション** を選択します。
1. **[リソース グループ]** で、次の手順のいずれかを行います。

    a. **[Use existing (既存のものを使用)]** を選択し、ドロップダウン リストから既存のリソース グループを選択します。
    
    b. **[新規作成]** を選択し、リソース グループの名前を入力します。リソース グループの詳細については、[リソース グループを使用した Azure リソースの管理](../azure-resource-manager/management/overview.md)に関する記事を参照してください。
    
1. **[バージョン]** で、 **[V2]** を選択します。
1. **[場所]** で、データ ファクトリの場所を選択します。 サポートされている場所のみがドロップダウン リストに表示されます。 データ ファクトリによって使用されるデータ ストア (Azure Storage、SQL Database など) やコンピューティング (Azure HDInsight など) は、他のリージョンに存在していてもかまいません。
1. **［作成］** を選択します
1. 作成が完了すると、その旨が通知センターに表示されます。 **[リソースに移動]** を選択して、Data factory ページに移動します。
1. **[Author & Monitor]\(作成と監視\)** を選択して、別のタブで Data Factory (UI) を起動します。

## <a name="create-a-pipeline-with-a-data-flow-activity"></a>データ フロー アクティビティが含まれるパイプラインの作成

この手順では、データ フロー アクティビティが含まれるパイプラインを作成します。

1. Azure Data Factory のホーム ページで、 **[Orchestrate]\(調整\)** を選択します。

   :::image type="content" source="./media/doc-common-process/get-started-page.png" alt-text="ADF のホーム ページを示すスクリーンショット。":::

1. パイプラインの **[全般]** タブで、パイプラインの **名前** として「**DeltaLake**」と入力します。
1. ファクトリの上部のバーで、 **[Data Flow のデバッグ]** スライダーをオンにスライドします。 デバッグ モードを使用すると、ライブ Spark クラスターに対する変換ロジックの対話型テストが可能になります。 Data Flow クラスターのウォームアップには 5 から 7 分かかるため、ユーザーが Data Flow の開発を計画している場合は、最初にデバッグを有効にすることをお勧めします。 詳細については、[デバッグ モード](concepts-data-flow-debug-mode.md)に関するページを参照してください。

    :::image type="content" source="media/tutorial-data-flow/dataflow1.png" alt-text="Data Flow アクティビティ":::
1. **[アクティビティ]** ウィンドウで、 **[移動と変換]** アコーディオンを展開します。 ウィンドウから **Data Flow** アクティビティをパイプライン キャンバスにドラッグ アンド ドロップします。

    :::image type="content" source="media/tutorial-data-flow/activity1.png" alt-text="Data Flow アクティビティをドロップできるパイプライン キャンバスを示すスクリーンショット。":::
1. **[Data Flow の追加]** ポップアップで、 **[新しい Data Flow の作成]** を選択し、データ フローに **DeltaLake** という名前を付けます。 終了したら、[完了] をクリックします。

    :::image type="content" source="media/tutorial-data-flow/activity2.png" alt-text="新しいデータ フローを作成するときにデータ フローの名前を指定する場所を示すスクリーンショット。":::

## <a name="build-transformation-logic-in-the-data-flow-canvas"></a>データ フロー キャンバスでの変換ロジックの作成

ソース データをすべて取得し (このチュートリアルでは、Parquet ファイル ソースを使用します)、シンク変換を使用して、データ レイク ETL に最も効果的なメカニズムで Parquet 形式のデータを配置します。

:::image type="content" source="media/data-flow/parts-final.png" alt-text="最終的なフロー":::

### <a name="tutorial-objectives"></a>チュートリアルの目標

1. 新しいデータ フローで任意のソース データセットを選択する
1. データ フローを使用してシンク データセットを効果的にパーティション分割する
1. ADLS Gen2 レイク フォルダーにパーティション分割されたデータを配置する

### <a name="start-from-a-blank-data-flow-canvas"></a>空のデータ フロー キャンバスから開始する

まず、ADLS Gen2 のデータ配置用に、以下で説明する各メカニズムのデータ フロー環境を設定してみましょう

1. [ソース変換] をクリックします。
1. 下部パネルのデータセットの横にある [新規] ボタンをクリックします。
1. サブネットを選択するか、新しいものを作成します。 このデモでは、User Data という Parquet データセットを使用します。
1. 派生列変換を追加します。 ここでは、必要なフォルダー名を動的に設定する方法として使用します。
1. シンク変換を追加します。
   
### <a name="hierarchical-folder-output"></a>階層フォルダーの出力

データ内で一意の値を使用して、レイク内のデータをパーティション分割するためのフォルダー階層を作成することは非常に一般的です。 これは、レイクおよび Spark (データ フローの背後にあるコンピューティング エンジン) でデータを整理して処理するために非常に最適な方法です。 ただし、この方法で出力を整理すると、パフォーマンスがわずかに低下します。 シンクでは、このメカニズムを使用すると、パイプラインの全体的なパフォーマンスがわずかに低下することが予想されます。

1. データ フロー デザイナーに戻り、先に作成したデータ フローを編集します。 [シンク変換] をクリックします。
1. [最適化] > [パーティション分割の設定] > [キー] の順にクリックします
1. 階層フォルダー構造を設定するために使用する列を選択します。
1. 下の例では、フォルダー名の列として年と月を使用しています。 結果として、フォルダーの形式は ```releaseyear=1990/month=8``` になります。
1. データ フロー ソース内のデータ パーティションにアクセスする場合は、```releaseyear``` の上の最上位フォルダーのみをポイントし、後続の各フォルダーにワイルドカード パターン (例: ```**/**/*.parquet```) を使用します
1. データ値を操作する場合、またはフォルダー名の合成値を生成する必要がある場合でも、派生列変換を使用して、フォルダー名に使用する値を作成します。

:::image type="content" source="media/data-flow/key-parts.png" alt-text="キーのパーティション分割":::
   
### <a name="name-folder-as-data-values"></a>データ値としてフォルダーに名前を付ける

キーや値のパーティション分割と同じベネフィットを得られない ADLS Gen2 を使用したレイク データに対し、わずかにパフォーマンスに優れたシンク手法が ```Name folder as column data``` です。 階層構造のキーのパーティション分割スタイルを使用すると、データ スライスを簡単に処理できますが、この手法はデータをより迅速に書き込むことができるフラット化されたフォルダー構造です。

1. データ フロー デザイナーに戻り、先に作成したデータ フローを編集します。 [シンク変換] をクリックします。
1. [最適化] > [パーティション分割の設定] > [Use current partitioning]\(現在のパーティション分割を使用\) の順にクリックします。
1. [設定] > [列データでフォルダーに名前を付ける] の順にクリックします。
1. フォルダー名を生成するために使用する列を選択します。
1. データ値を操作する場合、またはフォルダー名の合成値を生成する必要がある場合でも、派生列変換を使用して、フォルダー名に使用する値を作成します。

:::image type="content" source="media/data-flow/folders.png" alt-text="フォルダー オプション":::

### <a name="name-file-as-data-values"></a>データ値としてファイル名に名前を付ける

上記のチュートリアルに記載されている手法は、データ レイクにフォルダーのカテゴリを作成するための優れたユース ケースです。 これらの手法によって採用されている既定のファイル命名スキームは、Spark 実行プログラムのジョブ ID を使用することです。 場合によっては、データ フロー テキスト シンクで出力ファイルの名前を設定する必要があります。 この手法は、小さなファイルを使用する場合にのみ推奨されます。 パーティション ファイルを 1 つの出力ファイルにマージするプロセスは、実行時間の長いプロセスです。

1. データ フロー デザイナーに戻り、先に作成したデータ フローを編集します。 [シンク変換] をクリックします。
1. [最適化] > [パーティション分割の設定] > [単一パーティション] の順にクリックします。 これは単一パーティションの要件であり、ファイルがマージされると実行プロセスにボトルネックが作成されます。 このオプションは、小さなファイルに対してのみ推奨されます。
1. [設定] > [列データでファイルに名前を付ける] の順にクリックします。
1. ファイル名を生成するために使用する列を選択します。
1. データ値を操作する場合、またはファイル名の合成値を生成する必要がある場合でも、派生列変換を使用して、ファイル名に使用する値を作成します。

## <a name="next-steps"></a>次の手順

[データ フロー シンク](data-flow-sink.md)の詳細を確認する。
