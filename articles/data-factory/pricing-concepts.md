---
title: Azure Data Factory の価格を実例から理解する
description: このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います
author: shirleywangmsft
ms.author: shwang
ms.reviewer: jburchel
ms.service: data-factory
ms.subservice: pricing
ms.topic: conceptual
ms.date: 09/07/2021
ms.openlocfilehash: 45d8b3dea72555470059cb14b3268e4d9b4e9611
ms.sourcegitcommit: 677e8acc9a2e8b842e4aef4472599f9264e989e7
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/11/2021
ms.locfileid: "132319516"
---
# <a name="understanding-data-factory-pricing-through-examples"></a>Data Factory の価格を実例から理解する

[!INCLUDE[appliesto-adf-xxx-md](includes/appliesto-adf-xxx-md.md)]

このアーティクルでは、詳細例を用いて Azure Data Factory の価格モデルの説明と実演を行います。  もっと具体的なシナリオが必要な場合や、将来、サービスを使用する際のコストを見積もる場合は、[Azure 料金計算ツール](https://azure.microsoft.com/pricing/calculator/)も参照できます。

> [!NOTE]
> 次の例で使用されている価格は仮定的なものであり、実際の価格を暗示することを意図するものではありません。

## <a name="copy-data-from-aws-s3-to-azure-blob-storage-hourly"></a>データを AWS S3 から Azure Blob Storage に1時間に1度コピーする

このシナリオでは、データを AWS S3 から Azure Blob　Storage に1時間に1度コピーします。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーするデータの入力データセットのコピーアクティビティ。

2. Azure Storage 上のデータの出力データセット。

3. パイプラインを毎時実行するスケジュールトリガー。

   :::image type="content" source="media/pricing-concepts/scenario1.png" alt-text="図には、スケジュール トリガーを伴うパイプラインが示されています。パイプラインでは、コピー アクティビティが入力データセットに送信され、ここから AWS S3 のリンクされたサービスに送信されます。また、コピー アクティビティは出力データセットにも送信され、ここから Azure Storate のリンクされたサービスに送信されます。":::

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $0.16811**

- Data Factory の操作 = **$0.0001**
  - 読み取り/書き込み = 10\*0.00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 2\*0.000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.168**
  - アクティビティの実行 = 0.001\*2 = $0.002 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-azure-databricks-hourly"></a>データをコピーし、 Azure Databricks で 1 時間ごとに変換する

このシナリオでは、1 時間ごとのスケジュールでデータを AWS S3 から Azure Blob Storage にコピーし、 Azure Databricks でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3 からコピーされるデータの入力データセットと、Azure　Storage のデータの出力データセットを含む 1 つのコピーアクティビティです。
2. データ変換のための１つの Azure Databricks アクティビティ。
3. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

:::image type="content" source="media/pricing-concepts/scenario2.png" alt-text="図には、スケジュール トリガーを伴うパイプラインが示されています。パイプラインでは、コピー アクティビティが入力データセット、出力データセット、および Azure Databricks で実行される DataBricks アクティビティに送信されます。入力データセットは、AWS S3 のリンクされたサービスに送信されます。出力データセットは、Azure Storage のリンクされたサービスに送信されます。":::

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 3つのアクティビティの実行(トリガーの実行用に1つ、アクティビティの実行用に2つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 3 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 2 つ) |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.16916**

- Data Factory の操作 = **$0.00012**
  - 読み取り/書き込み = 11\*0.00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 3\*0.000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.16904**
  - アクティビティの実行 = 0.001\*3 = $0.003 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="copy-data-and-transform-with-dynamic-parameters-hourly"></a>データをコピーし動的パラメーターで1時間ごとに変換する

このシナリオでは、1時間ごとのスケジュールでデータを AWS S3 から Azure Blob Storage にコピーし、 Azure Databricks （スクリプト内に動的パラメーターのある）でデータを変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. AWS S3からコピーされるデータの入力データセット、Azure Storageのデータの出力データセットを含む1つのコピーアクティビティです。
2. 変換スクリプトにパラメーターを動的に渡すための1つの検索アクティビティ。
3. データ変換のための１つの Azure Databricks アクティビティ。
4. パイプラインを 1 時間ごとに実行する 1 つのスケジュールトリガー。

:::image type="content" source="media/pricing-concepts/scenario3.png" alt-text="図には、スケジュール トリガーを伴うパイプラインが示されています。パイプラインでは、コピー アクティビティが入力データセット、出力データセット、およびルックアップ アクティビティに送信されます。ルックアップ アクティビティは DataBricks アクティビティに送信され、このアクティビティは Azure Databricks で実行されます。入力データセットは、AWS S3 のリンクされたサービスに送信されます。出力データセットは、Azure Storage のリンクされたサービスに送信されます。":::

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 3 つの書き込み/読み取りエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 4 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 3 つ) |
| Data Assumption をコピーする: 実行時間 = 10 分 | 10 \* 4つの Azure 統合ランタイム (デフォルト DIU 設定 = 4) データ統合ユニットとコピーパフォーマンスの最適化についての詳細は、 [このアーティクル](copy-activity-performance.md)を参照してください |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 4 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 3 つ) |
| 検索アクティビティ Assumption を実行する: 実行時間 = 1 分 | 1 分間のパイプラインアクティビティの実行 |
| Databricksアクティビティ Assumption を実行する: 実行時間 = 10 分 | 10 分間の外部パイプラインアクティビティの実行 |

**シナリオ価格の合計: $0.17020**

- Data Factory の操作 = **$0.00013**
  - 読み取り/書き込み = 11\*0.00001 = $0.00011 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 4\*0.000005 = $0.00002 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインの統合 &amp; 実行 = **$0.17007**
  - アクティビティの実行 = 0.001\*4 = $0.004 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.166 (日割りの実行時間の 10 分間です。 $0.25/時間、 Azure 統合ランタイム)
  - パイプラインアクティビティ = $0.00003 （日割りの実行時間の1分間です。 $0.002/時間、 Azure 統合ランタイム)
  - 外部パイプラインアクティビティ = $0.000041 （日割の実行時間の10分間です。 $0.00025/時間、 Azure 統合ランタイム)

## <a name="run-ssis-packages-on-azure-ssis-integration-runtime"></a>Azure-SSIS 統合ランタイムで SSIS パッケージを実行する

Azure-SSIS 統合ランタイム (IR) は、Azure Data Factory (ADF) で SSIS パッケージを実行するための Azure 仮想マシン (VM) の特殊なクラスターです。 これは、プロビジョニングされると、そのユーザー専用になります。そのため、これを使用して SSIS パッケージを実行するかどうかにかかわらず、実行し続ける限り、他のすべての専用の Azure VM と同様に課金されます。 その実行コストについては、ADF ポータルのセットアップ ペインに時間単位の見積もりが表示されます。たとえば、次のようになります。  

:::image type="content" source="media/pricing-concepts/ssis-pricing-example.png" alt-text="SSIS の価格の例":::

上の例では、Azure-SSIS IR を 2 時間実行し続けると、**2 (時間) x 1.158 米ドル/時 = 2.316 米ドル** が課金されます。

Azure-SSIS IR の実行コストを管理するには、VM サイズをスケールダウンしたり、クラスター サイズをスケールインしたり、大幅な節約をもたらす Azure ハイブリッド特典 (AHB) オプションを使用して独自の SQL Server ライセンスを取得したりする ([Azure-SSIS IR の価格](https://azure.microsoft.com/pricing/details/data-factory/ssis/)に関するページを参照) か、または SSIS ワークロードを処理するために必要に応じていつでも、オンデマンドで、またはジャストインタイムに Azure-SSIS IR を起動および停止する ([Azure-SSIS IR の再構成](manage-azure-ssis-integration-runtime.md#to-reconfigure-an-azure-ssis-ir)および [Azure-SSIS IR のスケジュール設定](how-to-schedule-azure-ssis-integration-runtime.md)に関するページを参照) ことができます。

## <a name="using-mapping-data-flow-debug-for-a-normal-workday"></a>通常の平日にマッピング データ フロー デバッグを使用する

Sam は、データ エンジニアとして、毎日マッピング データ フローを設計、構築、テストする責任があります。 Sam は、朝に ADF UI にログインし、データ フローのデバッグ モードを有効にします。 デバッグ セッションの既定の TTL は、60 分です。 Sam は 1 日 8 時間働いているので、デバッグ セッションは期限切れになりません。 したがって、Sam のその日の料金は次のようになります。

**8 (時間) x 8 (コンピューティング最適化コア数) x $0.193 = $12.35**

同時に、別のデータ エンジニアの Chris も、データ プロファイルおよび ETL 設計作業のために ADF ブラウザー UI にログインします。 Chris は、Sam のように終日 ADF で作業するわけではありません。 Chris は、上記の Sam と同じ日の同じ期間に 1 時間、データ フロー デバッガーを使用する必要があるだけです。 デバッグの使用に対して Chris が負う料金は次のとおりです。

**1 (時間) x 8 (汎用コア数) x $0.274 = $2.19**

## <a name="transform-data-in-blob-store-with-mapping-data-flows"></a>マッピング データ フローを使用して BLOB ストア内のデータを変換する

このシナリオでは、時間単位のスケジュールで BLOB ストア内のデータを ADF マッピング データ フローで視覚的に変換します。

シナリオを実現させるには、次の項目を含むパイプラインを作成する必要があります:

1. 変換ロジックを含む Data Flow アクティビティ。

2. Azure Storage 上のデータの入力データセット。

3. Azure Storage 上のデータの出力データセット。

4. パイプラインを毎時実行するスケジュールトリガー。

| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 2つの読み取り/書き込みエンティティ  |
| データセットを作成する | 4 つの読み取り/書き込みエンティティ(データセットの作成用に 2 つ、リンクされたサービスの参照用に 2 つ) |
| パイプラインを作成する | 3 つの読み取り/書き込みエンティティ (パイプラインの作成用に 1 つ、データセットの参照用に 2 つ) |
| パイプラインを取得する | 1 つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 2 つのアクティビティの実行 (トリガーの実行用に 1 つ、アクティビティの実行用に 1 つ) |
| Data Flow の前提条件: 実行時間 = 10 分 + TTL 10 分 | 10 \* 16 コアの一般コンピューティング (TTL 10) |
| パイプライン Assumption を監視する: 1 つの実行のみが発生しました | 2 つの取得された監視実行レコード (パイプラインの実行用に 1 つ、アクティビティの実行用に 1 つ) |

**シナリオ価格の合計: $1.4631**

- Data Factory の操作 = **$0.0001**
  - 読み取り/書き込み = 10\*0.00001 = $0.0001 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 2\*0.000005 = $0.00001 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインのオーケストレーション &amp; 実行 = **$1.463**
  - アクティビティの実行 = 0.001\*2 = $0.002 [1 実行 = $1/1000 = 0.001]
  - データ フロー アクティビティ = $1.461: 20 分間の時間割り (実行時間 10 分 + TTL 10 分)。 Azure Integration Runtime で $0.274/時間、16 コアの一般コンピューティング

## <a name="data-integration-in-azure-data-factory-managed-vnet"></a>Azure Data Factory マネージド VNET でのデータ統合
このシナリオでは、Azure Blob Storage にある元のファイルを削除し、Azure SQL Database から Azure Blob Storage にデータをコピーします。 この実行を、異なるパイプラインで 2 回行います。 これら 2 つのパイプラインでの実行時間は重なっています。
:::image type="content" source="media/pricing-concepts/scenario-4.png" alt-text="Scenario4":::
シナリオを実現するには、次の項目を含む 2 つのパイプラインを作成する必要があります。
  - パイプライン アクティビティ – 削除アクティビティ。
  - Azure Blob Storage からコピーするデータに対する入力データセットのコピー アクティビティ。
  - Azure SQL Database 上のデータに対する出力データセット。
  - パイプラインを実行するためのスケジュール トリガー。


| **操作** | **タイプとユニット** |
| --- | --- |
| リンクされたサービスを作成する | 4 つの書き込み/読み取りエンティティ |
| データセットを作成する | 8 つの読み取り/書き込みエンティティ(データセットの作成用に 4 つ、リンクされたサービスの参照用に 4 つ) |
| パイプラインを作成する | 6 つの読み取り/書き込みエンティティ (パイプラインの作成用に 2 つ、データセットの参照用に 4 つ) |
| パイプラインを取得する | 2つの読み取り/書き込みエンティティ |
| パイプラインを実行する | 6 つのアクティビティ実行 (トリガーの実行用に 2 つ、アクティビティの実行用に 4 つ) |
| 削除アクティビティの実行: 各実行時間 = 5 分。最初のパイプラインでの削除アクティビティの実行は、10:00 AM UTC から 10:05 AM UTC までです。 2 番目のパイプラインでの削除アクティビティの実行は、10:02 AM UTC から 10:07 AM UTC までです。|マネージド VNET での合計 7 分のパイプライン アクティビティ実行。 パイプライン アクティビティでは、マネージド VNET で最大 50 個の同時実行がサポートされています。 パイプライン アクティビティには 60 分の有効期限 (TTL) があります|
| Data Assumption をコピーする: 各実行時間 = 10 分。最初のパイプラインでのコピー実行は、10:06 AM UTC から 10:15 AM UTC までです。 2 番目のパイプラインでのコピー アクティビティの実行は、10:08 AM UTC から 10:17 AM UTC までです。 | 10 * 4 つの Azure Integration Runtime (既定の DIU 設定 = 4) データ統合ユニットとコピー パフォーマンスの最適化についての詳細は、[こちらの記事](copy-activity-performance.md)を参照してください |
| パイプライン監視の仮定:実行が 2 回だけ発生 | 6 つの取得された監視実行レコード (パイプラインの実行用に 2 つ、アクティビティの実行用に 4 つ) |


**シナリオ価格の合計: $1.45523**

- Data Factory の操作 = $0.00023
  - 読み取り/書き込み = 20*0.00001 = $0.0002 [1 R/W = $0.50/50000 = 0.00001]
  - 監視 = 6*0.000005 = $0.00003 [1 監視 = $0.25/50000 = 0.000005]
- パイプラインのオーケストレーションと実行 = $1.455
  - アクティビティの実行 = 0.001*6 = $0.006 [1 実行 = $1/1000 = 0.001]
  - データ移動アクティビティ = $0.333 (実行時間 10 分に対する日割り。 $0.25/時間、 Azure 統合ランタイム)
  - パイプライン アクティビティ = $ 1.116 (実行時間 7 分に対する按分計算と 60 分の TTL)。 $1/時間、Azure Integration Runtime)

> [!NOTE] 
> これらの価格はあくまでも例です。

**FAQ**

Q:50 より多くのパイプライン アクティビティを実行する場合、これらのアクティビティを同時に実行することはできますか?

A:許可される同時実行パイプライン アクティビティは最大 50 個です。  51 番目のパイプライン アクティビティは、"空きスロット" が開放されるまでキューに登録されます。 外部アクティビティについても同じです。 許可される同時実行外部アクティビティは最大 800 個です。

## <a name="next-steps"></a>次のステップ

Azure Data Factory の価格を理解したところで、始めることができます!

- [Azure Data Factory UIを使用してData Factoryを作成する](quickstart-create-data-factory-portal.md)

- [Azure Data Factory の概要](introduction.md)

- [Azure Data Factory でのビジュアルの作成](author-visually.md)
