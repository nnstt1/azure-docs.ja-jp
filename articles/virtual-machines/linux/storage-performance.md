---
title: Azure Lsv2 シリーズの仮想マシン上でパフォーマンスを最適化する - ストレージ
description: Lsv2 シリーズの仮想マシン上でソリューションのパフォーマンスを最適化する方法をLinux を例にして紹介します。
author: laurenhughes
ms.service: virtual-machines
ms-subservice: vm-sizes-storage
ms.collection: linux
ms.topic: conceptual
ms.tgt_pltfrm: vm-linux
ms.workload: infrastructure-services
ms.date: 08/05/2019
ms.author: joelpell
ms.openlocfilehash: 5dd1e506b2eafff1d197a108dfa521dc89499925
ms.sourcegitcommit: 58d82486531472268c5ff70b1e012fc008226753
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 08/23/2021
ms.locfileid: "122691022"
---
# <a name="optimize-performance-on-the-lsv2-series-linux-virtual-machines"></a>Lsv2 シリーズ Linux 仮想マシン上でパフォーマンスを最適化する

**適用対象:** :heavy_check_mark: Linux VM :heavy_check_mark: フレキシブル スケール セット 

Lsv2 シリーズの仮想マシンは、幅広いアプリケーションや業界において、ローカル ストレージに高い I/O とスループットを必要とするさまざまなワークロードをサポートしています。  Lsv2 シリーズは、Cassandra、MongoDB、Cloudera、Redis などのビッグ データ、SQL、NoSQL データベース、データ ウェアハウス、大規模トランザクション データベースに最適です。

Lsv2 シリーズの仮想マシン (VM) の設計は、AMD EPYC™ 7551 プロセッサを最大限活用し、プロセッサ、メモリ、NVMe デバイス、VM の間で最善のパフォーマンスを実現できるようになっています。 Linux のパートナーと協力することで、Lsv2 シリーズのパフォーマンス向けに最適化されたいくかつのビルドを Azure Marketplace で入手できます。現在、以下のものがあります。

- Ubuntu 18.04
- Ubuntu 16.04
- RHEL 8.0
- Debian 9
- Debian 10

この記事では、ワークロードとアプリケーションで VM の設計に応じた最大限のパフォーマンスを実現するためのヒントと推奨事項を紹介します。 このページに掲載した情報は、最適化済みの Lsv2 イメージが Azure Marketplace に追加されるのに応じて随時更新していく予定です。

## <a name="amd-epyc-chipset-architecture"></a>AMD EPYC™ チップセットのアーキテクチャ

Lsv2 シリーズの VM では、Zen マイクロアーキテクチャをベースとする AMD EYPC™ サーバー プロセッサを使用しています。 AMD は、オンダイ、オンパッケージ、マルチパッケージの通信に利用が期待できる自らの NUMA モデルのスケーラブルなインターコネクトとして、Infinity Fabric (IF) for EYPC™ を開発しました。 NUMA が多くダイが少ない AMD のアーキテクチャは、Intel の最新型モノリシックダイ プロセッサで採用されている QPI (Quick-Path Interconnect) と UPI (Ultra-Path Interconnect) に比べると、パフォーマンス面でメリットと課題のどちらももたらしうる可能性を秘めています。 メモリの帯域幅と待ち時間の制約の影響が実際にどれほどのものになるかは、実行するワークロードの種類に応じて異なります。

## <a name="tips-to-maximize-performance"></a>パフォーマンス最大化のためのヒント

* ワークロードのためにカスタムの Linux GuestOS をアップロードする場合には、高速ネットワークが既定で **オフ** になっていることに注意してください。 高速ネットワークを有効にする場合は、VM の作成時に有効にすると、最善のパフォーマンスが得られます。

* Lsv2 シリーズ VM が稼働するハードウェアでは、I/O キュー ペア (QP) を 8 組備えた NVMe デバイスを使用しています。 NVMe デバイスの I/O キューはいずれも、実際には送信キューと完了キューがペアになっています。 NVMe ドライバーは、ラウンド ロビン方式のスケジュールに基づいて I/O を分散させ、この 8 組の I/O QP の利用を最適化するように設定してあります。 最大限のパフォーマンスを実現するためには、デバイスごとに釣り合いの取れたジョブを 8 件実行するようにしてください。

* アクティブなワークロードの最中に NVMe の管理者向けコマンド (NVMe の SMART 情報のクエリなど) と NVMe の I/O コマンドを混用することは避けてください。 Lsv2 NVMe デバイスは Hyper-V の NVMe Direct テクノロジを採用しており、NVMe の管理者向けコマンドが保留中の状態になると、"低速モード" に切り替わります。 このような事態が発生した場合、NVMe の I/O パフォーマンスが大幅に低下することがあります。

* アプリの NUMA アフィニティを決めるにあたっては、データ ドライブ用 VM からのレポートに表示されるデバイスの NUMA 情報 (すべて 0) を信用しないようにしてください。 パフォーマンス向上のために、可能であればワークロードを複数の CPU に分散させることをお勧めします。

* Lsv2 VM の NVMe デバイスの 1 組の I/O キュー ペアでサポートされるキューの深さは 1,024 です (Amazon i3 の QD の上限は 32 です)。 キューがいっぱいになり、パフォーマンスが低下する事態を防ぐため、(合成) ベンチマークのワークロードにおけるキューの深さは 1,024 以下にしてください。

## <a name="utilizing-local-nvme-storage"></a>ローカルの NVMe ストレージを使用する

Lsv2 VM にある 1.92 TB NVMe ディスク上のローカル ストレージは、エフェメラル ストレージです。 VM の標準的な再起動処理が正常に実行されている間は、ローカル NVMe ディスクにあるデータが保持されます。 VM の再デプロイ、割り当て解除、または削除を行った場合には、NVMe 上にデータが保持されません。 他の問題が原因となって VM またはその VM が稼働しているハードウェアが正常な状態ではなくなった場合には、データが保持されません。 このような事態が発生した場合には、以前のホストに存在するデータがすべて安全に消去されます。

計画メンテナンス業務の間などには、VM を別のホスト マシンに移行することが必要になることもあります。 [Scheduled Events](scheduled-events.md) では、計画メンテナンス業務の予定やハードウェア障害の見込みを確認できます。 Scheduled Events を使用して、メンテナンス業務や回復業務の予定の最新情報を確認するようにしてください。

計画メンテナンス イベントの際に、空のローカル ディスクを備えた新しいホストに VM を作り直す必要が生じた場合には、データを同期し直す必要があります (繰り返しになりますが、以前のホストに存在するデータはすべて安全に消去されます)。 これは、Lsv2 シリーズの VM が現在、ローカル NVMe ディスク上のライブ マイグレーションをサポートしていないことによるものです。

計画メンテナンスには 2 つのモードがあります。

### <a name="standard-vm-customer-controlled-maintenance"></a>Standard VM の顧客コントロール型メンテナンス

- 30 日の間に、VM が新しいホストに移行します。
- Lsv2 ローカル ストレージのデータが失われる可能性があるので、イベントの前にデータのバックアップを作成しておくことをお勧めします。

### <a name="automatic-maintenance"></a>自動メンテナンス

- お客様が顧客コントロール型メンテナンスを実施しなかったり、緊急処置が必要になったりした場合 (セキュリティに関するゼロデイ イベントなど) に発生します。
- お客様のデータの保持を目的としたものですが、VM のフリーズや再起動が発生するリスクがわずかに存在します。
- Lsv2 ローカル ストレージのデータが失われる可能性があるので、イベントの前にデータのバックアップを作成しておくことをお勧めします。

サービス イベントが近づいてきたら、コントロール型のメンテナンス プロセスを使って更新に最も都合の良いタイミングを選択してください。 イベントの前には、Premium ストレージ内のデータをバックアップしておくのも良いでしょう。 メンテナンス イベントが完了した後は、新しい Lsv2 VM のローカル NVMe ストレージにデータを戻すことができます。

ローカルの NVMe ディスクにあるデータが保持されるシナリオは次のとおりです。

- VM が正常な状態で稼働している。
- VM が (お客様または Azure により) 再起動中になっている。
- VM が一時停止している (割り当て解除せずに停止した)。
- 計画メンテナンス サービス業務の大多数。

お客様の保護のためにデータが安全に消去されるシナリオは次のとおりです。

- (お客様が) VM を再デプロイ、停止 (割り当て解除)、または削除した。
- ハードウェアの問題が原因で VM が異常な状態になっており、別のノードに復旧させる必要がある。
- 保守作業のために VM を別のホストに割り当て直す必要がある少数の計画メンテナンス サービス業務。

データをローカル ストレージにバックアップする際のオプションの詳細については、「[Azure IaaS ディスクのバックアップとディザスター リカバリー](../backup-and-disaster-recovery-for-azure-iaas-disks.md)」を参照してください。

## <a name="frequently-asked-questions"></a>よく寄せられる質問

* **Lsv2 シリーズの VM のデプロイを始めるにはどうすればよいでしょうか?**  
   他の VM と同じく、[ポータル](quick-create-portal.md)、[Azure CLI](quick-create-cli.md)、[PowerShell](quick-create-powershell.md) のいずれかを使って VM を作成します。

* **1 つの NVMe ディスクに障害が発生すると、そのホストにある VM すべてに障害が起こるのでしょうか?**  
   ハードウェア ノードでディスクの障害が検出された場合には、ハードウェアの状態が障害の存在を示すものに変わります。 このような事態が発生すると、そのノードにある VM はいずれも割り当てが解除され、別の正常なノードに移行します。 Lsv2 シリーズの VM の場合には、障害が発生したノードにあるお客様のデータも安全に消去されることになるので、新しいノードにお客様が自らデータを作り直す必要があります。 既に述べたとおり、Lsv2 でライブ マイグレーションが利用できるようになるまでは、VM が別のノードに移行する際に、障害が発生しているノードにあるデータも一緒に移行されます。

* **パフォーマンス向上のために rq_affinity に何か調整を加える必要はありますか?**  
   rq_affinity の設定は、1 秒あたりの入出力処理件数 (IOPS) の絶対最大値を使用している場合には、小さな調整にとどまります。 他のあらゆる点が正常に動作していることがわかったら、rq_affinity を 0 に設定してみて、何か変化があるかどうかを確認してください。

* **blk_mq の設定には変更が必要ですか?**  
   NVMe デバイスの場合、RHEL/CentOS 7.x により自動的に blk-mq が使用されます。 構成の変更や設定は必要ありません。 scsi_mod.use_blk_mq の設定は SCSI 専用です。この設定は、Lsv2 のプレビュー期間中に NVMe デバイスがゲスト VM 内で SCSI デバイスとして表示されていたために使われていたものです。 現在は NVMe デバイスが NVMe デバイスとして表示されるようになったため、SCSI の blk-mq の設定は関係がありません。

* **"fio" には変更が必要ですか?**  
   L64v2 と L80v2 サイズの VM 内で "fio" のようなパフォーマンス計測ツールを使って IOPS の最大値を取得するには、各 NVMe デバイスの "rq_affinity" を 0 に設定します。  たとえば、このコマンド ラインは、L80v2 VM 内の 10 台の NVMe デバイスすべてを対象に "rq_affinity" を 0 に設定するものです。

   ```console
   for i in `seq 0 9`; do echo 0 >/sys/block/nvme${i}n1/queue/rq_affinity; done
   ```

   このほか、最善のパフォーマンスが得られるのは、未加工 (パーティション分割をしておらず、ファイル システムがなく、RAID 0 を構成していないなど) の各 NVMe デバイスに対して直接 I/O を実行したときであるという点にご注意ください。テスト セッションを開始する前に、それぞれの NVMe デバイスで `blkdiscard` を実行し、構成が既知のフレッシュまたはクリーンな状態になっていることを確認してください。
   
## <a name="next-steps"></a>次のステップ

* Azure 上で[ストレージのパフォーマンスを高めるために最適化されたすべての VM](../sizes-storage.md) の仕様を確認してください。
