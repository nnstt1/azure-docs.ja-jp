---
title: 機械学習における差分プライバシー (プレビュー)
titleSuffix: Azure Machine Learning
description: 差分プライバシーとは何かについて説明すると共に、差分プライバシー システムでどのようにデータのプライバシーが維持されるかについて説明します。
author: luisquintanilla
ms.author: luquinta
ms.date: 10/21/2021
services: machine-learning
ms.service: machine-learning
ms.subservice: enterprise-readiness
ms.topic: conceptual
ms.custom: responsible-ml, mktng-kw-nov2021
ms.openlocfilehash: fc3606a4d893152b87fc5f5f47d987c16ce65724
ms.sourcegitcommit: 8946cfadd89ce8830ebfe358145fd37c0dc4d10e
ms.translationtype: HT
ms.contentlocale: ja-JP
ms.lasthandoff: 11/05/2021
ms.locfileid: "131842559"
---
# <a name="what-is-differential-privacy-in-machine-learning-preview"></a>機械学習における差分プライバシーとは (プレビュー)

機械学習における差分プライバシーとそのしくみについて説明します。

組織では、分析用に収集/使用するデータの量が増えるにつれて、プライバシーとセキュリティについての懸念も増していきます。 分析にはデータが必要です。 通常、機械学習モデルのトレーニングに使用されるデータが多いほど、モデルの精度は高くなります。 これらの分析に個人情報が使用される場合には、データのプライバシーを維持したまま、それらのデータが使用されるようにすることが特に重要となります。

## <a name="how-differential-privacy-works"></a>差分プライバシーのしくみ

差分プライバシーとは、個人のデータを安全かつプライベートに保つうえで役立つ、一連のシステムとプラクティスのことです。 機械学習ソリューションでは、規制遵守のために差分プライバシーが必要になる場合があります。

> [!div class="mx-imgBorder"]
> ![差分プライバシーの機械学習プロセス](./media/concept-differential-privacy/differential-privacy-machine-learning.jpg)

従来のシナリオでは、生データはファイルとデータベースに保存されます。 データを分析する場合、ユーザーは通常、生データを使用します。 しかし、この方法では個人のプライバシーを侵害する可能性があるため、問題があります。 差分プライバシーは、データに "ノイズ" やランダム性を追加し、ユーザーが個々のデータ ポイントを識別できないようにすることで、この問題に対処しようとするものです。 このようなシステムにより、少なくとも一定の防御性を提供することができます。 したがって、個人のプライバシーは、データの精度への影響を最小限に抑えて維持されます。

差分プライバシー システムでは、データは **クエリ** と呼ばれる要求を通じて共有されます。 ユーザーがデータのクエリを送信すると、**プライバシー メカニズム** と呼ばれる操作によって、要求されたデータにノイズが追加されます。 プライバシー メカニズムでは、生データではなく、*データの近似値* が返されます。 このようにプライバシーを維持した結果が、**レポート** に表示されます。 レポートは、2 つの部分で構成されます。実際の計算済みデータと、データがどのように作成されたかの説明です。

## <a name="differential-privacy-metrics"></a>差分プライバシーのメトリック

差分プライバシーでは、ユーザーによってレポートが無制限に生成され、最終的に機密データが漏洩するのを防ぐための措置が講じられます。 レポートのノイズがどの程度で、プライバシーがどの程度確保されるかという度合いは、**epsilon** という値によって測定されます。 epsilon は、ノイズやプライバシーの度合いに反比例します。 epsilon が低いほど、データはノイズが高く (プライベートに) なります。

epsilon の値は、負以外の値となります。 1 未満の値を指定すると、完全な防御性が得られます。 1 より大きい値にすると、実際のデータが危険にさらされる可能性がより高くなります。 差分プライバシーを備えた機械学習ソリューションを実装する場合、データの epsilon 値は 0 から 1 の間である必要があります。

epsilon に直接関連付けられているもう 1 つの値が、**delta** です。 delta は、レポートが完全にプライベートでない可能性を示す尺度です。 delta が高いほど、epsilon は大きくなります。 これらの値には相関関係があるため、epsilon のほうがより頻繁に使用されます。

## <a name="limit-queries-with-a-privacy-budget"></a>プライバシー予算でクエリを制限する

差分プライバシーでは、複数のクエリが許可されているシステムでプライバシーを維持するために、レート制限が定義されます。 この制限は、**プライバシー予算** と呼ばれます。 プライバシー予算は、複数のクエリによってデータが再作成されることを防ぎます。 プライバシー予算には epsilon (通常は 1 - 3 の範囲) が割り当てられ、これによって再識別のリスクが制限されます。 レポートが生成される際には、プライバシー予算によって、個々のレポートの epsilon 値と、すべてのレポートの集計が追跡されます。 プライバシー予算を使い果たすと、ユーザーはデータにアクセスできなくなります。 

## <a name="reliability-of-data"></a>データの信頼性

プライバシーの維持を目標とすることは必要ですが、データのユーザビリティと信頼性に関しては、あるトレードオフが存在します。 データ分析の場合、精度とは、サンプリング エラーによって生じる不確実性の尺度と考えることができます。 この不確実性は、特定の範囲内に収まる傾向があります。 一方、差分プライバシーの観点から見た場合、**精度** とは、データの信頼性を測る尺度となります。そしてこれは、プライバシー メカニズムによって生じる不確実性の影響を受けます。 つまり、ノイズやプライバシーのレベルが高いほど、データの epsilon、精度、および信頼性は低くなるということです。 

## <a name="open-source-differential-privacy-libraries"></a>オープンソースの差分プライバシー ライブラリ

SmartNoise は、差分プライバシーを備えた機械学習ソリューションを構築するためのコンポーネントを含む、オープンソースのプロジェクトです。 SmartNoise を構成している最上位レベルのコンポーネントは次のとおりです。

- SmartNoise コア ライブラリ
- SmartNoise SDK ライブラリ

### <a name="smartnoise-core"></a>SmartNoise コア

コア ライブラリには、差分プライバシー システムを実装するための、次のプライバシー メカニズムが含まれています。

|コンポーネント  |説明  |
|---------|---------|
|分析     | 任意の計算のグラフ記述。 |
|検証コントロール     | 分析を差分プライベートにするために必要な条件を確認して抽出するための、一連のツールを含んだ Rust ライブラリ。          |
|ランタイム     | 分析を実行するメディア。 参照ランタイムは Rust で記述されますが、ランタイムは、データのニーズに応じて任意の計算フレームワークを使用して記述できます (SQL や Spark など)。        |
|バインド     | 分析を構築するための言語バインドとヘルパー ライブラリ。 現在、SmartNoise では Python のバインドが提供されています。 |

### <a name="smartnoise-sdk"></a>SmartNoise SDK

システム ライブラリでは、表形式データとリレーショナル データを操作するための、次のツールとサービスが提供されています。

|コンポーネント  |説明  |
|---------|---------|
|[データ アクセス]     | SQL クエリをインターセプトして処理し、レポートを生成するライブラリ。 このライブラリは Python で実装されます。次の ODBC および DBAPI データソースがサポートされています。<ul><li>PostgreSQL</li><li>SQL Server</li><li>Spark</li><li>Preston</li><li>Pandas</li></ul>|
|サービス     | 共有データソースに対する要求やクエリを処理する REST エンドポイントを提供する実行サービス。 このサービスは、さまざまな delta 値と epsilon 値 (異種要求とも呼ばれます) を含んだ要求に対して動作する、差分プライバシー モジュールを構成できるように設計されています。 この参照実装は、関連付けられたデータに対するクエリから受ける追加的な影響に対応します。 |
|エバリュエーター     | プライバシー違反、精度、およびバイアスをチェックするストキャスティクス エバリュエーター。 エバリュエーターでは、次のテストがサポートされています。 <ul><li>プライバシー テスト - レポートが差分プライバシーの条件に準拠しているかどうかを判断します。</li><li>正確性テスト - レポートの信頼性が、95% の信頼度レベルで上限と下限の範囲内であるかどうかを測定します。</li><li>ユーティリティ テスト - レポートの信頼度の範囲がデータに対して十分に近いかどうかを判断しながら、プライバシーを最大限に高めます。</li><li>バイアス テスト - レポートの分布を測定してクエリの繰り返しをチェックし、不均衡を防ぎます</li></ul> |

## <a name="next-steps"></a>次のステップ

機械学習における差分プライバシーについて学びます。 

 - Azure Machine Learning で[差分プライバシー システム](how-to-differential-privacy.md)を構築する方法。

 - SmartNoise のコンポーネントの詳細については、[SmartNoise コア](https://github.com/opendifferentialprivacy/smartnoise-core)、[SmartNoise SDK](https://github.com/opendifferentialprivacy/smartnoise-sdk)、[SmartNoise サンプル](https://github.com/opendifferentialprivacy/smartnoise-samples)に関する GitHub リポジトリを参照してください。